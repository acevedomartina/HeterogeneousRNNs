{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f0d337",
   "metadata": {},
   "source": [
    "Running both oscillations and sequences in parallelized code. I "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f02de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2b74d",
   "metadata": {},
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31497da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76fea6b2",
   "metadata": {},
   "source": [
    "# Oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating targets...\n",
      "✓ Targets generated\n",
      "\n",
      "Total simulations (vt/vrest configurations): 4\n",
      "pqif values per simulation: 5\n",
      "Seeds per pqif: 50\n",
      "Parallelization strategy: 5 pqif values × 50 seeds = 250 parallel processes per simulation\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 1: vt=0, vrest=-8.5\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 157.0min remaining: 235.4min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 157.1min remaining: 104.7min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 157.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 1 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 2: vt=0, vrest=-12.3\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 155.9min remaining: 233.8min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 156.0min remaining: 104.0min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 156.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 2 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 3: vt=0, vrest=-17\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 156.9min remaining: 235.4min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 157.0min remaining: 104.6min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 157.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 3 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 4: vt=0, vrest=-22\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 157.0min remaining: 235.5min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 157.1min remaining: 104.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 4 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 157.3min finished\n"
     ]
    }
   ],
   "source": [
    "# ========== Oscillations (parallelized) ==========\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "####### Global parameters #######\n",
    "# Neurons\n",
    "N = 200                 # Number of nodes (neurons)\n",
    "N2 = int(N/2)           # Half\n",
    "\n",
    "# Synaptic connections\n",
    "p = 0.3                 # Probability of connection (non-zero elements in the weight matrix)\n",
    "gsyn = 0.5              # Initial synaptic strength\n",
    "alpha = 0.25            # Weight regularization parameter\n",
    "# Dynamics\n",
    "dt = 0.1                # Time step (time scale 10 ms)\n",
    "itmax = 1000            # Number of iterations, where 1000 --> 1 sec\n",
    "sigman = 1              # Noise standard deviation --> Noise in the dynamics\n",
    "# Stimulus\n",
    "itstim = 200            # Stimulation time\n",
    "amp_corriente = 20      # Stimulus intensity\n",
    "amp0 = 4                # Used in target. Changed from 8 to 4, in order to have the same current amplitudes as in the pre-training case for both oscillations and sequences\n",
    "# Training\n",
    "nloop = 16              # Number of loops, 0: pre-training, last: post-training\n",
    "nloop_train = 10        # Last training loop\n",
    "cant_seed = 50          # Independent simulations\n",
    "ts = 5                  # \n",
    "b = 1 / ts              # adaptation parameter for r - In evolution of r, dr/dt = -b * r, b is magnitude\n",
    "ftrain = 1              # Fraction of neurons to train\n",
    "\n",
    "####### File organization functions #######\n",
    "def crear_subcarpeta(nombre_carpeta, nombre_subcarpeta):\n",
    "    '''\n",
    "    Joins path components (folder and subfolder) if not existing already\n",
    "\n",
    "    nombre_carpeta: Folder name\n",
    "    nombre_subcarpeta: Subfolder name\n",
    "    '''\n",
    "    subcarpeta_path_total = (os.path.join(nombre_carpeta, nombre_subcarpeta))\n",
    "    if not os.path.exists(subcarpeta_path_total):\n",
    "        os.makedirs(subcarpeta_path_total)\n",
    "    return subcarpeta_path_total\n",
    "\n",
    "def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "    '''\n",
    "    Joins path components (folder and subfolder) if not existing already\n",
    "\n",
    "    carpeta_padre: Parent folder name\n",
    "    nombre_subcarpeta: Subfolder name\n",
    "    '''\n",
    "    ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "    if not os.path.exists(ruta):\n",
    "        os.makedirs(ruta)\n",
    "    return ruta\n",
    "\n",
    "def crear_carpetas(num_simulacion): \n",
    "    '''\n",
    "    Main simulation folder (simulation_idx with all subfolders)\n",
    "    '''\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "\n",
    "    # Subfolders within the simulation\n",
    "    sub_act = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "\n",
    "    return nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes\n",
    "\n",
    "def crear_archivo_parametros(filename_resultados, num_simulacion, nombre_carpeta, b, vt, vrest):\n",
    "    '''    \n",
    "    Save simulation parameters to file\n",
    "    '''\n",
    "    data_parametros = {\n",
    "        'N': [N],\n",
    "        'p': [p],\n",
    "        'gsyn': [gsyn],\n",
    "        'nloop': [nloop],\n",
    "        'nloop_train':[nloop_train],\n",
    "        'cant_seed': [cant_seed],\n",
    "        'dt': [dt],\n",
    "        'itmax': [itmax],\n",
    "        'itstim': [itstim],\n",
    "        'amp_corriente': [amp_corriente],\n",
    "        'amp0': [amp0],\n",
    "        'ftrain': [ftrain],\n",
    "        'alpha': [alpha],\n",
    "        'sigman': [sigman],\n",
    "        'vt': [vt],\n",
    "        'b': [b],\n",
    "        'vrest': [vrest],\n",
    "        'results_file': [filename_resultados],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_parametros)\n",
    "    filename_parametros = f'simulation_{num_simulacion}_parameters.csv'\n",
    "    csv_parametros_path = os.path.join(nombre_carpeta, filename_parametros)\n",
    "    df.to_csv(csv_parametros_path, index=False)\n",
    "\n",
    "####### Function to generate target patterns #######\n",
    "\n",
    "def generate_target(romega1, romega2, amp0):\n",
    "    '''\n",
    "    Generates oscillatory target with theta and gamma (to replicate oscillatory frequencies in the MEC)\n",
    "    ----------\n",
    "    romega1: theta\n",
    "    romega2: gamma (5*theta)\n",
    "    amp0: \n",
    "    '''\n",
    "    target=np.zeros((N,itmax))  # Neuron (row) activity over timesteps (columns)\n",
    "    amp=np.random.uniform(size=N)*amp0  #\n",
    "    phase=np.random.uniform(0,2*np.pi,size=N)\n",
    "    indices = [i for i in range(N)]\n",
    "    indices = np.random.permutation(indices) # Indices to identify which neuron is assigned each frequency\n",
    "    \n",
    "    romega_vec = np.zeros(N)  # (N,)\n",
    "    \n",
    "    for i in range(N2):\n",
    "        # Assigning frequencies to indices\n",
    "        romega_vec[indices[i]]= romega1\n",
    "        romega_vec[indices[i+N2]]=romega2\n",
    "        #romega_vec now has frequencies assigned to indices\n",
    "    \n",
    "    omega=romega_vec*2*np.pi/itmax  # \n",
    "\n",
    "    for it in range(itmax):\n",
    "        target[:,it]=amp*np.cos(it*omega+phase) \n",
    "            \n",
    "    return target, amp, phase, omega, romega_vec, amp0\n",
    "\n",
    "\n",
    "def save_target(target, phase, omega, romega_vec, amp, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "    # Save target parameters and values to CSV\n",
    "    data = {'Neurona': range(N), 'Fase': phase, 'Frecuencia': omega, 'romega': romega_vec, 'Amplitud': amp, 'amp0': amp0}\n",
    "    df = pd.DataFrame(data)\n",
    "    nombre_archivo = f'simulation_{num_simulacion}_targets_parameters.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "    df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "    target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "    nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "    target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "\n",
    "def guardar_matriz_csv(matriz, nombre_archivo):\n",
    "    ''' \n",
    "    Make matrix\n",
    "    '''\n",
    "    with open(nombre_archivo, 'w', newline='') as archivo_csv:\n",
    "        escritor_csv = csv.writer(archivo_csv)\n",
    "        for fila in matriz:\n",
    "            fila_lista = [str(elemento) for elemento in fila.flat]  # rows\n",
    "            escritor_csv.writerow(fila_lista)\n",
    "\n",
    "\n",
    "####### Dynamics and learning functions #######\n",
    "\n",
    "def dynamics(x_var,r_var,I_var,nqif, b):\n",
    "    \"\"\"\n",
    "    Compute derivatives of neuron state and adaptation variable.\n",
    "    \n",
    "    Inputs:\n",
    "        x_var   : internal state of neurons\n",
    "        r_var   : output firing rate or adaptation variable\n",
    "        I_var   : total input to neurons (external + recurrent)\n",
    "        nqif    : number of QIF neurons at the start of the array\n",
    "        b       : adaptation parameter for r\n",
    "        \n",
    "    Outputs:\n",
    "        dx      : derivative of neuron state\n",
    "        dr      : derivative of adaptation/firing rate\n",
    "    \"\"\"\n",
    "    # Initialize dx (derivative of state) as zeros for all neurons\n",
    "    dx=np.zeros(N)\n",
    "        # -----------------------------\n",
    "    # Step 1: Add stochastic noise to inputs\n",
    "    # -----------------------------\n",
    "    # LIF neurons: neurons from index nqif onward\n",
    "    # Generate Gaussian noise for LIF neurons\n",
    "    I_noise_lif = np.random.randn(N - nqif)*sigman \n",
    "    # QIF neurons: neurons from index 0 to nqif-1\n",
    "    # Generate Gaussian noise for QIF neurons  \n",
    "    I_noise_qif = np.random.randn(nqif)*sigman\n",
    "    # -----------------------------\n",
    "    # Step 2: Compute derivative for LIF neurons\n",
    "    # -----------------------------\n",
    "    # LIF dynamics: dx/dt = -x + I + noise\n",
    "    # Applies to neurons with indices nqif:N\n",
    "    dx[nqif:] = -x_var[nqif:] + I_var[nqif:] + I_noise_lif\n",
    "    # -----------------------------\n",
    "    # Step 3: Compute derivative for QIF neurons\n",
    "    # -----------------------------\n",
    "    # QIF dynamics: dx/dt = 1 - cos(x) + I*(1 + cos(x)) + noise\n",
    "    # This is a phase-based neuron model, x represents phase in [0, 2π)\n",
    "    dx[:nqif] = 1 - np.cos(x_var[:nqif]) + I_var[:nqif]*(1 + np.cos(x_var[:nqif])) + I_noise_qif\n",
    "    # -----------------------------\n",
    "    # Step 4: Compute derivative for adaptation variable r\n",
    "    # -----------------------------\n",
    "    # Adaptation decays exponentially: dr/dt = -b * r\n",
    "    dr = -b*r_var \n",
    "    return dx,dr\n",
    "\n",
    "\n",
    "def detect(x,xnew,rnew,nspike,nqif, b, vt, vrest):\n",
    "    # LIF spike detection\n",
    "    ispike_lif=np.where(x[nqif:]<vt) and np.where(xnew[nqif:]>vt)\n",
    "    ispike_lif=ispike_lif[0]+nqif\n",
    "    if(len(ispike_lif)>0):\n",
    "        rnew[ispike_lif[:]] = rnew[ispike_lif[:]] + b\n",
    "        xnew[ispike_lif[:]] = vrest\n",
    "        nspike[ispike_lif[:]] = nspike[ispike_lif[:]] + 1\n",
    "    # QIF spike detection\n",
    "    dpi=np.mod(np.pi - np.mod(x,2*np.pi),2*np.pi)  # distance to pi\n",
    "    ispike_qif=np.where((xnew[:nqif]-x[:nqif])>0) and np.where((xnew[:nqif]-x[:nqif]-dpi[:nqif])>0)\n",
    "    if(len(ispike_qif)>0):\n",
    "        rnew[ispike_qif[:]] = rnew[ispike_qif[:]] + b\n",
    "        nspike[ispike_qif[:]] = nspike[ispike_qif[:]] + 1\n",
    "    return xnew,rnew,nspike\n",
    "\n",
    "def evolution(x, r, Iext, w, nqif, it, dt, iout, nspike, b, vt, vrest):\n",
    "    II = np.squeeze(np.asarray(Iext[:, it]))\n",
    "    v = w.dot(r.T).A1\n",
    "    dx, dr = dynamics(x, r, II + v, nqif, b)\n",
    "    xnew = x + dt * dx / 2\n",
    "    rnew = r + dt * dr / 2\n",
    "    dx, dr = dynamics(xnew, rnew, II + v, nqif, b)\n",
    "    xnew = x + dt * dx\n",
    "    rnew = r + dt * dr\n",
    "    xnew, rnew, nspike = detect(x, xnew, rnew, nspike, nqif, b, vt, vrest)\n",
    "    x, r = np.copy(xnew), np.copy(rnew)\n",
    "\n",
    "    return x, r, nspike, r[iout], II, v\n",
    "\n",
    "\n",
    "def initialize_connectivity_matrix(N, p, gsyn):\n",
    "    w = sparse.random(N, N, p, data_rvs=np.random.randn).todense()\n",
    "    np.fill_diagonal(w, 0)  # No autapses\n",
    "    w *= gsyn / np.sqrt(p * N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        i0 = np.where(w[i, :])[1]\n",
    "        if len(i0) > 0:\n",
    "            av0 = np.sum(w[i, i0]) / len(i0)\n",
    "            w[i, i0] -= av0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def initialize_neurons(N):\n",
    "    x = np.random.uniform(size=N) * 2 * np.pi\n",
    "    r = np.zeros(N)\n",
    "    nspike = np.zeros(N)\n",
    "    return x, r, nspike\n",
    "\n",
    "def initialize_training(N, w):\n",
    "    # Initialize correlation matrices for RLS learning\n",
    "    nind=np.zeros(N).astype('int')\n",
    "    idx=[]\n",
    "    P=[]\n",
    "    for i in range(N):\n",
    "        ind=np.where(w[i,:])[1]\n",
    "        nind[i]=len(ind)\n",
    "        idx.append(ind)\n",
    "        P.append(np.identity(nind[i])/alpha)   \n",
    "    return P, idx\n",
    "\n",
    "def currents(N, itmax):\n",
    "    Iext=np.zeros((N,itmax))\n",
    "    Ibac=amp_corriente*(2*np.random.uniform(size=N)-1)\n",
    "    Iext[:, :itstim] = Ibac[:, None]  # Vectorized assignment\n",
    "    return Iext\n",
    "\n",
    "\n",
    "def learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer):\n",
    "    error = target[:, it:it + 1] - w @ r.reshape(N, 1)\n",
    "    for i in range(N):\n",
    "        ri = r[idx[i]].reshape(len(idx[i]), 1)  \n",
    "        k1 = P[i] @ ri\n",
    "        k2 = ri.T @ P[i]\n",
    "        den = 1 + ri.T @ k1\n",
    "        P[i] -= (k1 @ k2) / den\n",
    "        dw = error[i, 0] * P[i] @ r[idx[i]]\n",
    "        w[i, idx[i]] += dw\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        modt_value = it + iloop * itmax\n",
    "        modw_value = np.log(np.linalg.norm(w) / norm_w0)\n",
    "        csv_writer.writerow([modt_value, modw_value])\n",
    "        \n",
    "    return w, P\n",
    "\n",
    "\n",
    "####### Motifs and dimensionality calculations #######\n",
    "            \n",
    "def motifs(w,gsyn,N):\n",
    "    w=w-np.mean(w)\n",
    "    \n",
    "    ww=np.matmul(w,w)\n",
    "    wtw=np.matmul(w.T,w)\n",
    "    wwt=np.matmul(w,w.T)\n",
    "    \n",
    "    sigma2=np.trace(wwt)/N\n",
    "    \n",
    "    tau_rec=np.trace(ww)\n",
    "    tau_rec/=sigma2*N\n",
    "    \n",
    "    tau_div=np.sum(wwt)-np.trace(wwt)\n",
    "    tau_div/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_con=np.sum(wtw)-np.trace(wtw)\n",
    "    tau_con/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_chn=2*(np.sum(ww)-np.trace(ww))\n",
    "    tau_chn/=sigma2*N*(N-1)\n",
    "    \n",
    "    return sigma2,tau_rec,tau_div,tau_con,tau_chn\n",
    "\n",
    "\n",
    "####### Parallelized simulation functions #######\n",
    "\n",
    "def run_single_seed(seed, pqif, num_simulacion, vt, vrest, target, \n",
    "                    N, N2, p, gsyn, nloop, nloop_train,\n",
    "                    dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                    b, iout, nombre_carpeta, sub_pesos, sub_corrientes, \n",
    "                    sub_inputs, sub_outputs, sub_nspikes):\n",
    "    \"\"\"\n",
    "    Run complete simulation for a single seed\n",
    "    This function is parallelized over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate nqif based on proportion of QIF neurons\n",
    "    nqif = int(N * pqif)\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    # Initialize network\n",
    "    x, r, nspike = initialize_neurons(N)\n",
    "    Iext = currents(N, itmax)\n",
    "    \n",
    "    # Save external current\n",
    "    path_Iext = os.path.join(nombre_carpeta, \n",
    "                            f'simulation_{num_simulacion}_Iext_pqif_{pqif}_seed_{seed}.csv')\n",
    "    guardar_matriz_csv(Iext, path_Iext)\n",
    "    \n",
    "    # Initialize connectivity\n",
    "    w = initialize_connectivity_matrix(N, p, gsyn)\n",
    "    norm_w0 = np.linalg.norm(w)\n",
    "    P, idx = initialize_training(N, w)\n",
    "    \n",
    "    # Prepare file for weight evolution tracking\n",
    "    filename_dw = os.path.join(nombre_carpeta, \n",
    "                              f'simulation_{num_simulacion}_dw_pqif_{pqif}_seed_{seed}.csv')\n",
    "    \n",
    "    # Storage for results across all loops\n",
    "    seed_results = []\n",
    "    corrientes_buffer = []\n",
    "    \n",
    "    with open(filename_dw, mode='w', newline='') as file_dw:\n",
    "        csv_writer_dw = csv.writer(file_dw)\n",
    "        csv_writer_dw.writerow(['modt', 'modw'])\n",
    "        \n",
    "        # Main training loop\n",
    "        for iloop in range(nloop):\n",
    "            \n",
    "            # Pre-allocate arrays for this loop\n",
    "            outputs_loop = []\n",
    "            inputs_loop = []\n",
    "            nspikes_loop = []\n",
    "            \n",
    "            # Define output paths\n",
    "            path_inputs = os.path.join(sub_inputs, \n",
    "                                      f'simulation_{num_simulacion}_inputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_nspikes = os.path.join(sub_nspikes, \n",
    "                                       f'simulation_{num_simulacion}_nspikes_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_outputs = os.path.join(sub_outputs, \n",
    "                                       f'simulation_{num_simulacion}_outputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            \n",
    "            # Time evolution for this loop\n",
    "            for it in range(itmax):\n",
    "                nspike = np.zeros(N)\n",
    "                \n",
    "                x, r, nspike, rout, II, v = evolution(x, r, Iext, w, nqif, it, dt, \n",
    "                                                     iout, nspike, b, vt=vt, vrest=vrest)\n",
    "                \n",
    "                entrada = II + v\n",
    "                \n",
    "                # Accumulate data in memory (more efficient than writing each iteration)\n",
    "                outputs_loop.append(rout)\n",
    "                inputs_loop.append(entrada)\n",
    "                nspikes_loop.append(nspike)\n",
    "                \n",
    "                # Record currents at specific time points in specific loops\n",
    "                if iloop in [nloop_train + 1, nloop - 1] and it % 20 == 0:\n",
    "                    corrientes_buffer.append([pqif, seed, iloop, it, \n",
    "                                            II[0], v[0], II[1], v[1], \n",
    "                                            II[N2+1], v[N2+1], II[N2+2], v[N2+2]])\n",
    "                \n",
    "                # Apply learning rule during training period\n",
    "                if iloop > 0 and iloop <= nloop_train and int(it > itstim):\n",
    "                    w, P = learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer_dw)\n",
    "            \n",
    "            # Save all data for this loop (single write per loop)\n",
    "            np.savetxt(path_inputs, np.array(inputs_loop), delimiter=',')\n",
    "            np.savetxt(path_nspikes, np.array(nspikes_loop), delimiter=',')\n",
    "            np.savetxt(path_outputs, np.array(outputs_loop), delimiter=',')\n",
    "            \n",
    "            # Calculate network motifs\n",
    "            sigma2, tau_rec, tau_div, tau_con, tau_chn = motifs(w, gsyn, N)\n",
    "            \n",
    "            # Save weight matrix at specific loops\n",
    "            if iloop == 0 or iloop == (nloop_train + 1):\n",
    "                path_w_seed = os.path.join(sub_pesos, \n",
    "                                          f'simulation_{num_simulacion}_connectivity_pqif_{pqif}_iloop_{iloop}_seed_{seed}')\n",
    "                guardar_matriz_csv(w, path_w_seed)\n",
    "            \n",
    "            # Store results for this loop\n",
    "            seed_results.append([pqif, seed, iloop, sigma2, tau_rec, \n",
    "                               tau_div, tau_con, tau_chn])\n",
    "    \n",
    "    return seed_results, corrientes_buffer\n",
    "\n",
    "\n",
    "def run_pqif_simulation(pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0, \n",
    "                        N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                        dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                        b, iout):\n",
    "    \"\"\"\n",
    "    Run simulation for specific pqif value, parallelizing over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simulation {num_simulacion} - Processing pqif = {pqif}\")\n",
    "    print(f\"vt={vt}, vrest={vrest}\")\n",
    "    print(f\"Parallelizing over {cant_seed} seeds\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get folder paths (already created)\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    sub_act = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "    \n",
    "    # Results file path\n",
    "    filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "    csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "    \n",
    "    # Prepare currents file header\n",
    "    path_currents_seed = os.path.join(sub_corrientes, \n",
    "                                       f'simulation_{num_simulacion}_currents_pqif_{pqif}.csv')\n",
    "    with open(path_currents_seed, mode='w', newline='') as file_:\n",
    "        writer_ = csv.writer(file_)\n",
    "        writer_.writerow(['pqif', 'seed', 'iloop', 'it', 'II_0', 'v_0', \n",
    "                        'II_1', 'v_1', 'II_N2+1', 'v_N2+1', 'II_N2+2', 'v_N2+2'])\n",
    "    \n",
    "    # Parallelize over seeds\n",
    "    results = Parallel(n_jobs=cant_seed, verbose=5)(\n",
    "        delayed(run_single_seed)(\n",
    "            seed, pqif, num_simulacion, vt, vrest, target,\n",
    "            N, N2, p, gsyn, nloop, nloop_train,\n",
    "            dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "            b, iout, nombre_carpeta, sub_pesos, sub_corrientes,\n",
    "            sub_inputs, sub_outputs, sub_nspikes\n",
    "        )\n",
    "        for seed in range(cant_seed)\n",
    "    )\n",
    "    \n",
    "    # Consolidate results from all seeds\n",
    "    # results is a list of (seed_results, corrientes_buffer) tuples\n",
    "    all_seed_results = []\n",
    "    all_corrientes = []\n",
    "    for seed_results, corrientes_buffer in results:\n",
    "        all_seed_results.extend(seed_results)\n",
    "        all_corrientes.extend(corrientes_buffer)\n",
    "    \n",
    "    # Write all results to file (append mode for this pqif)\n",
    "    with open(csv_file_path, 'a', newline='') as file_res:\n",
    "        writer_res = csv.writer(file_res)\n",
    "        writer_res.writerows(all_seed_results)\n",
    "    \n",
    "    # Write all currents to file\n",
    "    if all_corrientes:\n",
    "        with open(path_currents_seed, 'a', newline='') as f_corr:\n",
    "            writer_corr = csv.writer(f_corr)\n",
    "            writer_corr.writerows(all_corrientes)\n",
    "    \n",
    "    print(f\"✓ pqif={pqif} completed for simulation {num_simulacion}\\n\")\n",
    "    \n",
    "    return num_simulacion\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    iout = np.linspace(0, N, num=N, endpoint=False).astype('int')\n",
    "    \n",
    "    # Generate target pattern once (shared across all simulations)\n",
    "    print(\"Generating targets...\")\n",
    "    target, phase, amp, omega, romega_vec, amp0 = generate_target(romega1=1, romega2=5, amp0=amp0)\n",
    "    print(\"✓ Targets generated\\n\")\n",
    "    \n",
    "    # Define pqif values to simulate\n",
    "    pqif_values = [0, 0.25, 0.5, 0.75, 1]  # pqif values are added here\n",
    "    \n",
    "    # Define vt/vrest configurations (defines simulation number)\n",
    "    # Each simulation uses different reset parameters for LIF neurons\n",
    "    # QIF neurons always use vt=None, vrest=None (handled in dynamics)\n",
    "    configs = [\n",
    "        {'vt': 0, 'vrest': -8.5},  # Simulation 1\n",
    "        {'vt': 0, 'vrest': -12.3},  # Simulation 2\n",
    "        {'vt': 0, 'vrest': -17},    # Simulation 3\n",
    "        {'vt': 0, 'vrest': -22},  # Simulation 4\n",
    "\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total simulations (vt/vrest configurations): {len(configs)}\")\n",
    "    print(f\"pqif values per simulation: {len(pqif_values)}\")\n",
    "    print(f\"Seeds per pqif: {cant_seed}\")\n",
    "    print(f\"Parallelization strategy: {len(pqif_values)} pqif values × {cant_seed} seeds = {len(pqif_values)*cant_seed} parallel processes per simulation\\n\")\n",
    "    \n",
    "    # Iterate over each vt/vrest configuration\n",
    "    for num_simulacion, config in enumerate(configs, start=1):\n",
    "        vt = config['vt']\n",
    "        vrest = config['vrest']\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# STARTING SIMULATION {num_simulacion}: vt={vt}, vrest={vrest}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        # Create folders and files for this simulation\n",
    "        nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes = crear_carpetas(num_simulacion)\n",
    "        \n",
    "        # Save targets for each pqif (all share the same target)\n",
    "        for pqif in pqif_values:\n",
    "            save_target(target, phase=phase, omega=omega, romega_vec=romega_vec, \n",
    "                       amp=amp, amp0=amp0, num_simulacion=num_simulacion, \n",
    "                       nombre_carpeta=nombre_carpeta, pqif=pqif)\n",
    "        \n",
    "        # Create parameters file\n",
    "        filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "        crear_archivo_parametros(filename_resultados, num_simulacion, \n",
    "                                nombre_carpeta, b, vt, vrest=vrest)\n",
    "        \n",
    "        # Create results file with header\n",
    "        csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['pqif', 'seed', 'nloop', 'sigma2', 'tau_rec',\n",
    "                           'tau_div', 'tau_con', 'tau_chn'])\n",
    "        \n",
    "        # Parallelize over pqif for this simulation\n",
    "        # Each pqif will internally parallelize over seeds\n",
    "        results = Parallel(n_jobs=len(pqif_values), verbose=10)(\n",
    "            delayed(run_pqif_simulation)(\n",
    "                pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0,\n",
    "                N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                b, iout\n",
    "            )\n",
    "            for pqif in pqif_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ✓ SIMULATION {num_simulacion} COMPLETED\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0009f",
   "metadata": {},
   "source": [
    "# Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91ecd825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating targets...\n",
      "✓ Targets generated\n",
      "\n",
      "Total simulations (vt/vrest configurations): 4\n",
      "pqif values per simulation: 5\n",
      "Seeds per pqif: 50\n",
      "Parallelization strategy: 5 pqif values × 50 seeds = 250 parallel processes per simulation\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 9: vt=0, vrest=-8.5\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 154.2min remaining: 231.4min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 154.4min remaining: 102.9min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 154.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 9 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 10: vt=0, vrest=-12.3\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 153.9min remaining: 230.9min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 154.2min remaining: 102.8min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 154.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 10 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 11: vt=0, vrest=-17\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 154.0min remaining: 231.0min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 154.1min remaining: 102.7min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 154.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 11 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 12: vt=0, vrest=-22\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 153.8min remaining: 230.7min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 153.9min remaining: 102.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 12 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 154.2min finished\n"
     ]
    }
   ],
   "source": [
    "# ========== Sequences (parallelized) ==========\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "####### Global parameters #######\n",
    "N = 200\n",
    "N2 = int(N/2)\n",
    "p = 0.3\n",
    "gsyn = 0.5\n",
    "alpha = 0.25\n",
    "dt = 0.1\n",
    "itmax = 1000\n",
    "sigman = 1\n",
    "itstim = 200\n",
    "amp_corriente = 20\n",
    "amp0 = 4 # changed from 8 to 4\n",
    "sg_index = 0.15 \n",
    "nloop = 16\n",
    "nloop_train = 10\n",
    "cant_seed = 50\n",
    "ts = 5\n",
    "b = 1 / ts\n",
    "ftrain = 1\n",
    "\n",
    "####### File organization functions #######\n",
    "## DUPLICATE\n",
    "# def crear_subcarpeta(nombre_carpeta, nombre_subcarpeta):\n",
    "#     subcarpeta_path_total = (os.path.join(nombre_carpeta, nombre_subcarpeta))\n",
    "#     if not os.path.exists(subcarpeta_path_total):\n",
    "#         os.makedirs(subcarpeta_path_total)\n",
    "#     return subcarpeta_path_total\n",
    "\n",
    "\n",
    "# def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "#     ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "#     if not os.path.exists(ruta):\n",
    "#         os.makedirs(ruta)\n",
    "#     return ruta\n",
    "\n",
    "# # def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "# #     ruta = Path(carpeta_padre) / nombre_subcarpeta\n",
    "# #     # ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "# #     if not ruta.exists():\n",
    "# #         ruta.mkdir(parents=True, exist_ok=True)\n",
    "# #     return ruta\n",
    "\n",
    "# def crear_carpetas(num_simulacion): \n",
    "#     # Main simulation folder\n",
    "#     # nombre_carpeta = ONEDRIVE_BASE / f\"simulation_{num_simulacion}\"\n",
    "#     nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "#     if not nombre_carpeta.exists():\n",
    "#         # os.makedirs(nombre_carpeta)\n",
    "#         nombre_carpeta.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def crear_subcarpeta(nombre_carpeta, nombre_subcarpeta):\n",
    "    subcarpeta_path_total = (os.path.join(nombre_carpeta, nombre_subcarpeta))\n",
    "    if not os.path.exists(subcarpeta_path_total):\n",
    "        os.makedirs(subcarpeta_path_total)\n",
    "    return subcarpeta_path_total\n",
    "\n",
    "\n",
    "def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "    ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "    if not os.path.exists(ruta):\n",
    "        os.makedirs(ruta)\n",
    "    return ruta\n",
    "\n",
    "def crear_carpetas(num_simulacion): \n",
    "    # Main simulation folder\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "\n",
    "    # Subfolders within the simulation\n",
    "    sub_act = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "\n",
    "    return nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes\n",
    "\n",
    "\n",
    "def crear_archivo_parametros(filename_resultados, num_simulacion, nombre_carpeta, b, vt, vrest):\n",
    "    # Save simulation parameters to file\n",
    "    data_parametros = {\n",
    "        'N': [N],\n",
    "        'p': [p],\n",
    "        'gsyn': [gsyn],\n",
    "        'nloop': [nloop],\n",
    "        'nloop_train':[nloop_train],\n",
    "        'cant_seed': [cant_seed],\n",
    "        'dt': [dt],\n",
    "        'itmax': [itmax],\n",
    "        'itstim': [itstim],\n",
    "        'amp_corriente': [amp_corriente],\n",
    "        'amp0': [amp0],\n",
    "        'ftrain': [ftrain],\n",
    "        'alpha': [alpha],\n",
    "        'sigman': [sigman],\n",
    "        'vt': [vt],\n",
    "        'b': [b],\n",
    "        'vrest': [vrest],\n",
    "        'results_file': [filename_resultados],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_parametros)\n",
    "    filename_parametros = f'simulation_{num_simulacion}_parameters.csv'\n",
    "    csv_parametros_path = os.path.join(nombre_carpeta, filename_parametros)\n",
    "    df.to_csv(csv_parametros_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "########### New target generation ###########\n",
    "# def generate_target(sg_index, amp0, pqif):\n",
    "\n",
    "#     target = np.zeros(shape=(N, itmax))\n",
    "#     # Periodic sequence target\n",
    "\n",
    "#     gg = np.zeros(N)\n",
    "#     sg = sg_index * N            # Gaussian width (relative to system size)\n",
    "#     omegagauss = 0.2             # Temporal shift velocity\n",
    "\n",
    "#     neuron_permutation = np.random.permutation(N)\n",
    "\n",
    "#     for i in range(N):\n",
    "#         gg[i] = amp0 * np.exp(-(i - N/2)**2 / (2 * sg**2))\n",
    "\n",
    "#     neuron_permutation = np.random.permutation(N)\n",
    "\n",
    "#     for it in range(itmax):\n",
    "#         gg_shifted = np.roll(gg, int(omegagauss * it))\n",
    "#         target[:, it] = gg_shifted[neuron_permutation]\n",
    "\n",
    "#     target_df = pd.DataFrame(target.T, columns=[f'Neuron_{i}' for i in range(N)])\n",
    "#     target_filename = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "#     target_csv_path = os.path.join(nombre_carpeta, target_filename)\n",
    "#     target_df.to_csv(target_csv_path, index=False)\n",
    "\n",
    "#     parameters_data = {\n",
    "#         'sg_index': sg_index,\n",
    "#         'omegagauss': omegagauss,\n",
    "#         'amp0': amp0,\n",
    "#         'sequence_order': [neuron_permutation]\n",
    "#     }\n",
    "#     parameters_df = pd.DataFrame(parameters_data, index=[0])\n",
    "#     parameters_filename = f'simulation_{num_simulacion}_targets_parameters.csv'\n",
    "#     parameters_csv_path = os.path.join(nombre_carpeta, parameters_filename)\n",
    "#     parameters_df.to_csv(parameters_csv_path, index=False)\n",
    "\n",
    "#     return target, parameters_data\n",
    "\n",
    "def generate_target(sg_index, amp0):\n",
    "\n",
    "    target = np.zeros(shape=(N, itmax))\n",
    "    # Periodic sequence target\n",
    "\n",
    "    gg = np.zeros(N)\n",
    "    sg = sg_index * N            # Gaussian width (relative to system size)\n",
    "    omegagauss = 0.2             # Temporal shift velocity\n",
    "\n",
    "    neuron_permutation = np.random.permutation(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        gg[i] = amp0 * np.exp(-(i - N/2)**2 / (2 * sg**2))  # Target\n",
    "\n",
    "    neuron_permutation = np.random.permutation(N)  # For permutating\n",
    "\n",
    "    for it in range(itmax):\n",
    "        gg_shifted = np.roll(gg, int(omegagauss * it))\n",
    "        target[:, it] = gg_shifted[neuron_permutation]\n",
    "\n",
    "    # target_df = pd.DataFrame(target.T, columns=[f'Neuron_{i}' for i in range(N)])\n",
    "    # target_filename = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "    # target_csv_path = os.path.join(nombre_carpeta, target_filename)\n",
    "    # target_df.to_csv(target_csv_path, index=False)\n",
    "\n",
    "    parameters_data = {  # Save parameters in its own dataframe\n",
    "        'sg_index': sg_index,\n",
    "        'omegagauss': omegagauss,\n",
    "        'amp0': amp0,\n",
    "        'sequence_order': [neuron_permutation]\n",
    "    }\n",
    "    # parameters_df = pd.DataFrame(parameters_data, index=[0])\n",
    "    # parameters_filename = f'simulation_{num_simulacion}_targets_parameters.csv'\n",
    "    # parameters_csv_path = os.path.join(nombre_carpeta, parameters_filename)\n",
    "    # parameters_df.to_csv(parameters_csv_path, index=False)\n",
    "\n",
    "    return target, parameters_data\n",
    "\n",
    "def save_target(target, parameters_data, num_simulacion, nombre_carpeta, pqif):\n",
    "    target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "    nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "    target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "    data = parameters_data\n",
    "    df = pd.DataFrame(data, index=[0])\n",
    "    nombre_archivo = f'simulation_{num_simulacion}_targets_parametros.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "    df.to_csv(csv_target_path, index=False)\n",
    "            \n",
    "    return target \n",
    "\n",
    "# ####### OLD Function to generate target patterns #######\n",
    "\n",
    "# ### Sequences version (I believe this also saves target)\n",
    "# # Needs to change to english\n",
    "\n",
    "\n",
    "# def generate_target(sg_index, amp0):\n",
    "\n",
    "#     target = np.zeros(shape=(N,itmax))\n",
    "#     #para secuencias periodicas\n",
    "\n",
    "#     gg=np.zeros(N)\n",
    "#     sg=sg_index*N            # ancho de la gaussiana. trelativo al tamanio del sistema\n",
    "#     omegagauss=0.1       # velocidad de desplazamiento\n",
    "#     for i in range(N):\n",
    "#         gg[i]=amp0*np.exp(-(i-N/2)**2/(2*sg**2))\n",
    "#     for it in range(itmax):\n",
    "#         target[:,it]=np.roll(gg,int(omegagauss*it))\n",
    "\n",
    "#     return target, omegagauss\n",
    "\n",
    "# def save_target(target, omegagauss, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "# def save_target(target, phase, omega, romega_vec, amp, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "#     target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "#     nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "#     csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "#     target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "#     data = {'sg_index': sg_index, 'omegagauss': omegagauss, 'amp0': amp0}\n",
    "#     df = pd.DataFrame(data, index=[0])\n",
    "#     nombre_archivo = f'simulation_{num_simulacion}_targets_parametros.csv'\n",
    "#     csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "#     df.to_csv(csv_target_path, index=False)\n",
    "            \n",
    "#     return target \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def guardar_matriz_csv(matriz, nombre_archivo):\n",
    "    with open(nombre_archivo, 'w', newline='') as archivo_csv:\n",
    "        escritor_csv = csv.writer(archivo_csv)\n",
    "        for fila in matriz:\n",
    "            fila_lista = [str(elemento) for elemento in fila.flat]\n",
    "            escritor_csv.writerow(fila_lista)\n",
    "\n",
    "\n",
    "####### Dynamics and learning functions #######\n",
    "\n",
    "def dynamics(x_var,r_var,I_var,nqif, b):\n",
    "    dx=np.zeros(N)\n",
    "    I_noise_lif = np.random.randn(N - nqif)*sigman \n",
    "    I_noise_qif = np.random.randn(nqif)*sigman\n",
    "    # LIF neurons\n",
    "    dx[nqif:] = -x_var[nqif:] + I_var[nqif:] + I_noise_lif\n",
    "    # QIF neurons\n",
    "    dx[:nqif] = 1 - np.cos(x_var[:nqif]) + I_var[:nqif]*(1 + np.cos(x_var[:nqif])) + I_noise_qif\n",
    "    dr = -b*r_var\n",
    "    return dx,dr\n",
    "\n",
    "\n",
    "def detect(x,xnew,rnew,nspike,nqif, b, vt, vrest):\n",
    "    # LIF spike detection\n",
    "    ispike_lif=np.where(x[nqif:]<vt) and np.where(xnew[nqif:]>vt)\n",
    "    ispike_lif=ispike_lif[0]+nqif\n",
    "    if(len(ispike_lif)>0):\n",
    "        rnew[ispike_lif[:]] = rnew[ispike_lif[:]] + b\n",
    "        xnew[ispike_lif[:]] = vrest\n",
    "        nspike[ispike_lif[:]] = nspike[ispike_lif[:]] + 1\n",
    "    # QIF spike detection\n",
    "    dpi=np.mod(np.pi - np.mod(x,2*np.pi),2*np.pi)  # distance to pi\n",
    "    ispike_qif=np.where((xnew[:nqif]-x[:nqif])>0) and np.where((xnew[:nqif]-x[:nqif]-dpi[:nqif])>0)\n",
    "    if(len(ispike_qif)>0):\n",
    "        rnew[ispike_qif[:]] = rnew[ispike_qif[:]] + b\n",
    "        nspike[ispike_qif[:]] = nspike[ispike_qif[:]] + 1\n",
    "    return xnew,rnew,nspike\n",
    "\n",
    "def evolution(x, r, Iext, w, nqif, it, dt, iout, nspike, b, vt, vrest):\n",
    "    II = np.squeeze(np.asarray(Iext[:, it]))\n",
    "    v = w.dot(r.T).A1\n",
    "    dx, dr = dynamics(x, r, II + v, nqif, b)\n",
    "    xnew = x + dt * dx / 2\n",
    "    rnew = r + dt * dr / 2\n",
    "    dx, dr = dynamics(xnew, rnew, II + v, nqif, b)\n",
    "    xnew = x + dt * dx\n",
    "    rnew = r + dt * dr\n",
    "    xnew, rnew, nspike = detect(x, xnew, rnew, nspike, nqif, b, vt, vrest)\n",
    "    x, r = np.copy(xnew), np.copy(rnew)\n",
    "\n",
    "    return x, r, nspike, r[iout], II, v\n",
    "\n",
    "\n",
    "def initialize_connectivity_matrix(N, p, gsyn):\n",
    "    w = sparse.random(N, N, p, data_rvs=np.random.randn).todense()\n",
    "    np.fill_diagonal(w, 0)  # No autapses\n",
    "    w *= gsyn / np.sqrt(p * N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        i0 = np.where(w[i, :])[1]\n",
    "        if len(i0) > 0:\n",
    "            av0 = np.sum(w[i, i0]) / len(i0)\n",
    "            w[i, i0] -= av0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def initialize_neurons(N):\n",
    "    x = np.random.uniform(size=N) * 2 * np.pi\n",
    "    r = np.zeros(N)\n",
    "    nspike = np.zeros(N)\n",
    "    return x, r, nspike\n",
    "\n",
    "def initialize_training(N, w):\n",
    "    # Initialize correlation matrices for RLS learning\n",
    "    nind=np.zeros(N).astype('int')\n",
    "    idx=[]\n",
    "    P=[]\n",
    "    for i in range(N):\n",
    "        ind=np.where(w[i,:])[1]\n",
    "        nind[i]=len(ind)\n",
    "        idx.append(ind)\n",
    "        P.append(np.identity(nind[i])/alpha)   \n",
    "    return P, idx\n",
    "\n",
    "def currents(N, itmax):\n",
    "    Iext=np.zeros((N,itmax))\n",
    "    Ibac=amp_corriente*(2*np.random.uniform(size=N)-1)\n",
    "    Iext[:, :itstim] = Ibac[:, None]  # Vectorized assignment\n",
    "    return Iext\n",
    "\n",
    "\n",
    "def learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer):\n",
    "    error = target[:, it:it + 1] - w @ r.reshape(N, 1)\n",
    "    for i in range(N):\n",
    "        ri = r[idx[i]].reshape(len(idx[i]), 1)\n",
    "        k1 = P[i] @ ri\n",
    "        k2 = ri.T @ P[i]\n",
    "        den = 1 + ri.T @ k1\n",
    "        P[i] -= (k1 @ k2) / den\n",
    "        dw = error[i, 0] * P[i] @ r[idx[i]]\n",
    "        w[i, idx[i]] += dw\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        modt_value = it + iloop * itmax\n",
    "        modw_value = np.log(np.linalg.norm(w) / norm_w0)\n",
    "        csv_writer.writerow([modt_value, modw_value])\n",
    "        \n",
    "    return w, P\n",
    "\n",
    "\n",
    "####### Motifs and dimensionality calculations #######\n",
    "            \n",
    "def motifs(w,gsyn,N):\n",
    "    w=w-np.mean(w)\n",
    "    \n",
    "    ww=np.matmul(w,w)\n",
    "    wtw=np.matmul(w.T,w)\n",
    "    wwt=np.matmul(w,w.T)\n",
    "    \n",
    "    sigma2=np.trace(wwt)/N\n",
    "    \n",
    "    tau_rec=np.trace(ww)\n",
    "    tau_rec/=sigma2*N\n",
    "    \n",
    "    tau_div=np.sum(wwt)-np.trace(wwt)\n",
    "    tau_div/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_con=np.sum(wtw)-np.trace(wtw)\n",
    "    tau_con/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_chn=2*(np.sum(ww)-np.trace(ww))\n",
    "    tau_chn/=sigma2*N*(N-1)\n",
    "    \n",
    "    return sigma2,tau_rec,tau_div,tau_con,tau_chn\n",
    "\n",
    "\n",
    "####### Parallelized simulation functions #######\n",
    "\n",
    "def run_single_seed(seed, pqif, num_simulacion, vt, vrest, target, \n",
    "                    N, N2, p, gsyn, nloop, nloop_train,\n",
    "                    dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                    b, iout, nombre_carpeta, sub_pesos, sub_corrientes, \n",
    "                    sub_inputs, sub_outputs, sub_nspikes):\n",
    "    \"\"\"\n",
    "    Run complete simulation for a single seed\n",
    "    This function is parallelized over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate nqif based on proportion of QIF neurons\n",
    "    nqif = int(N * pqif)\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    # Initialize network\n",
    "    x, r, nspike = initialize_neurons(N)\n",
    "    Iext = currents(N, itmax)\n",
    "    \n",
    "    # Save external current\n",
    "    path_Iext = os.path.join(nombre_carpeta, \n",
    "                            f'simulation_{num_simulacion}_Iext_pqif_{pqif}_seed_{seed}.csv')\n",
    "    guardar_matriz_csv(Iext, path_Iext)\n",
    "    \n",
    "    # Initialize connectivity\n",
    "    w = initialize_connectivity_matrix(N, p, gsyn)\n",
    "    norm_w0 = np.linalg.norm(w)\n",
    "    P, idx = initialize_training(N, w)\n",
    "    \n",
    "    # Prepare file for weight evolution tracking\n",
    "    filename_dw = os.path.join(nombre_carpeta, \n",
    "                              f'simulation_{num_simulacion}_dw_pqif_{pqif}_seed_{seed}.csv')\n",
    "    \n",
    "    # Storage for results across all loops\n",
    "    seed_results = []\n",
    "    corrientes_buffer = []\n",
    "    \n",
    "    with open(filename_dw, mode='w', newline='') as file_dw:\n",
    "        csv_writer_dw = csv.writer(file_dw)\n",
    "        csv_writer_dw.writerow(['modt', 'modw'])\n",
    "        \n",
    "        # Main training loop\n",
    "        for iloop in range(nloop):\n",
    "            \n",
    "            # Pre-allocate arrays for this loop\n",
    "            outputs_loop = []\n",
    "            inputs_loop = []\n",
    "            nspikes_loop = []\n",
    "            \n",
    "            # Define output paths\n",
    "            path_inputs = os.path.join(sub_inputs, \n",
    "                                      f'simulation_{num_simulacion}_inputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_nspikes = os.path.join(sub_nspikes, \n",
    "                                       f'simulation_{num_simulacion}_nspikes_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_outputs = os.path.join(sub_outputs, \n",
    "                                       f'simulation_{num_simulacion}_outputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            \n",
    "            # Time evolution for this loop\n",
    "            for it in range(itmax):\n",
    "                nspike = np.zeros(N)\n",
    "                \n",
    "                x, r, nspike, rout, II, v = evolution(x, r, Iext, w, nqif, it, dt, \n",
    "                                                     iout, nspike, b, vt=vt, vrest=vrest)\n",
    "                \n",
    "                entrada = II + v\n",
    "                \n",
    "                # Accumulate data in memory (more efficient than writing each iteration)\n",
    "                outputs_loop.append(rout)\n",
    "                inputs_loop.append(entrada)\n",
    "                nspikes_loop.append(nspike)\n",
    "                \n",
    "                # Record currents at specific time points in specific loops\n",
    "                if iloop in [nloop_train + 1, nloop - 1] and it % 20 == 0:\n",
    "                    corrientes_buffer.append([pqif, seed, iloop, it, \n",
    "                                            II[0], v[0], II[1], v[1], \n",
    "                                            II[N2+1], v[N2+1], II[N2+2], v[N2+2]])\n",
    "                \n",
    "                # Apply learning rule during training period\n",
    "                if iloop > 0 and iloop <= nloop_train and int(it > itstim):\n",
    "                    w, P = learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer_dw)\n",
    "            \n",
    "            # Save all data for this loop (single write per loop)\n",
    "            np.savetxt(path_inputs, np.array(inputs_loop), delimiter=',')\n",
    "            np.savetxt(path_nspikes, np.array(nspikes_loop), delimiter=',')\n",
    "            np.savetxt(path_outputs, np.array(outputs_loop), delimiter=',')\n",
    "            \n",
    "            # Calculate network motifs\n",
    "            sigma2, tau_rec, tau_div, tau_con, tau_chn = motifs(w, gsyn, N)\n",
    "            \n",
    "            # Save weight matrix at specific loops\n",
    "            if iloop == 0 or iloop == (nloop_train + 1):\n",
    "                path_w_seed = os.path.join(sub_pesos, \n",
    "                                          f'simulation_{num_simulacion}_connectivity_pqif_{pqif}_iloop_{iloop}_seed_{seed}')\n",
    "                guardar_matriz_csv(w, path_w_seed)\n",
    "            \n",
    "            # Store results for this loop\n",
    "            seed_results.append([pqif, seed, iloop, sigma2, tau_rec, \n",
    "                               tau_div, tau_con, tau_chn])\n",
    "    \n",
    "    return seed_results, corrientes_buffer\n",
    "\n",
    "\n",
    "def run_pqif_simulation(pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0, \n",
    "                        N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                        dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                        b, iout):\n",
    "    \"\"\"\n",
    "    Run simulation for specific pqif value, parallelizing over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simulation {num_simulacion} - Processing pqif = {pqif}\")\n",
    "    print(f\"vt={vt}, vrest={vrest}\")\n",
    "    print(f\"Parallelizing over {cant_seed} seeds\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get folder paths (already created)\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    sub_act = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "    \n",
    "    # Results file path\n",
    "    filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "    csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "    \n",
    "    # Prepare currents file header\n",
    "    path_currents_seed = os.path.join(sub_corrientes, \n",
    "                                       f'simulation_{num_simulacion}_currents_pqif_{pqif}.csv')\n",
    "    with open(path_currents_seed, mode='w', newline='') as file_:\n",
    "        writer_ = csv.writer(file_)\n",
    "        writer_.writerow(['pqif', 'seed', 'iloop', 'it', 'II_0', 'v_0', \n",
    "                        'II_1', 'v_1', 'II_N2+1', 'v_N2+1', 'II_N2+2', 'v_N2+2'])\n",
    "    \n",
    "    # Parallelize over seeds\n",
    "    results = Parallel(n_jobs=cant_seed, verbose=5)(\n",
    "        delayed(run_single_seed)(\n",
    "            seed, pqif, num_simulacion, vt, vrest, target,\n",
    "            N, N2, p, gsyn, nloop, nloop_train,\n",
    "            dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "            b, iout, nombre_carpeta, sub_pesos, sub_corrientes,\n",
    "            sub_inputs, sub_outputs, sub_nspikes\n",
    "        )\n",
    "        for seed in range(cant_seed)\n",
    "    )\n",
    "    \n",
    "    # Consolidate results from all seeds\n",
    "    # results is a list of (seed_results, corrientes_buffer) tuples\n",
    "    all_seed_results = []\n",
    "    all_corrientes = []\n",
    "    for seed_results, corrientes_buffer in results:\n",
    "        all_seed_results.extend(seed_results)\n",
    "        all_corrientes.extend(corrientes_buffer)\n",
    "    \n",
    "    # Write all results to file (append mode for this pqif)\n",
    "    with open(csv_file_path, 'a', newline='') as file_res:\n",
    "        writer_res = csv.writer(file_res)\n",
    "        writer_res.writerows(all_seed_results)\n",
    "    \n",
    "    # Write all currents to file\n",
    "    if all_corrientes:\n",
    "        with open(path_currents_seed, 'a', newline='') as f_corr:\n",
    "            writer_corr = csv.writer(f_corr)\n",
    "            writer_corr.writerows(all_corrientes)\n",
    "    \n",
    "    print(f\"✓ pqif={pqif} completed for simulation {num_simulacion}\\n\")\n",
    "    \n",
    "    return num_simulacion\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    iout = np.linspace(0, N, num=N, endpoint=False).astype('int')\n",
    "    \n",
    "    # Generate target pattern once (shared across all simulations)\n",
    "    print(\"Generating targets...\")\n",
    "    # target, omegagauss = generate_target(sg_index, amp0=amp0)\n",
    "    # target = generate_target(num_simulacion, nombre_carpeta,sg_index, amp0, pqif=pqif)\n",
    "    # num_simulacion = 9  # Same target for all\n",
    "    # target, parameters_data = generate_target(num_simulacion, nombre_carpeta,sg_index, amp0, pqif=pqif)\n",
    "    target, parameters_data = generate_target(sg_index, amp0)\n",
    "\n",
    "    print(\"✓ Targets generated\\n\")\n",
    "    \n",
    "    # Define pqif values to simulate\n",
    "    pqif_values = [0, 0.25, 0.5, 0.75, 1]  # pqif values are added here\n",
    "    \n",
    "    # Define vt/vrest configurations (defines simulation number)\n",
    "    # Each simulation uses different reset parameters for LIF neurons\n",
    "    # QIF neurons always use vt=None, vrest=None (handled in dynamics)\n",
    "    configs = [\n",
    "        {'vt': 0, 'vrest': -8.5},  # Simulation 9\n",
    "        {'vt': 0, 'vrest': -12.3},  # Simulation 10\n",
    "        {'vt': 0, 'vrest': -17},    # Simulation 11\n",
    "        {'vt': 0, 'vrest': -22},  # Simulation 12\n",
    "\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total simulations (vt/vrest configurations): {len(configs)}\")\n",
    "    print(f\"pqif values per simulation: {len(pqif_values)}\")\n",
    "    print(f\"Seeds per pqif: {cant_seed}\")\n",
    "    print(f\"Parallelization strategy: {len(pqif_values)} pqif values × {cant_seed} seeds = {len(pqif_values)*cant_seed} parallel processes per simulation\\n\")\n",
    "    \n",
    "    # Iterate over each vt/vrest configuration\n",
    "    for num_simulacion, config in enumerate(configs, start=9):  # I tried to make start = 9 \n",
    "        vt = config['vt']\n",
    "        vrest = config['vrest']\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# STARTING SIMULATION {num_simulacion}: vt={vt}, vrest={vrest}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        # Create folders and files for this simulation\n",
    "        nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes = crear_carpetas(num_simulacion)\n",
    "        \n",
    "        \n",
    "        # I commented this out because in the sequernces code, it seems to save target as part of generate_target function\n",
    "\n",
    "        phase = None\n",
    "        amp = None\n",
    "        omega = None\n",
    "        romega_vec = None  # They are not used in sequences I believe, but I'm not sure where to change the code, so I try with setting them to None for now\n",
    "\n",
    "        # Save targets for each pqif (all share the same target)\n",
    "        for pqif in pqif_values:\n",
    "            save_target(target, parameters_data, num_simulacion=num_simulacion, \n",
    "                       nombre_carpeta=nombre_carpeta, pqif=pqif)\n",
    "        \n",
    "        # Create parameters file\n",
    "        filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "        crear_archivo_parametros(filename_resultados, num_simulacion, \n",
    "                                nombre_carpeta, b, vt, vrest=vrest)\n",
    "        \n",
    "        # Create results file with header\n",
    "        csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['pqif', 'seed', 'nloop', 'sigma2', 'tau_rec',\n",
    "                           'tau_div', 'tau_con', 'tau_chn'])\n",
    "        \n",
    "\n",
    "        # Parallelize over pqif for this simulation\n",
    "        # Each pqif will internally parallelize over seeds\n",
    "        results = Parallel(n_jobs=len(pqif_values), verbose=10)(\n",
    "            delayed(run_pqif_simulation)(\n",
    "                pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0,\n",
    "                N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                b, iout\n",
    "            )\n",
    "            for pqif in pqif_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ✓ SIMULATION {num_simulacion} COMPLETED\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cfe5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.94700902 1.94700902 1.94700902 ... 2.02534247 2.02534247 2.02534247]\n",
      " [3.964602   3.964602   3.964602   ... 3.94482847 3.94482847 3.94482847]\n",
      " [0.03630035 0.03630035 0.03630035 ... 0.0327548  0.0327548  0.0327548 ]\n",
      " ...\n",
      " [0.7915948  0.7915948  0.7915948  ... 0.74508185 0.74508185 0.74508185]\n",
      " [2.66790724 2.66790724 2.66790724 ... 2.74763023 2.74763023 2.74763023]\n",
      " [1.23458984 1.23458984 1.23458984 ... 1.17242252 1.17242252 1.17242252]]\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
