{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f0d337",
   "metadata": {},
   "source": [
    "Run both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f02de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc7c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating targets...\n",
      "✓ Targets generated\n",
      "\n",
      "Total simulations (vt/vrest configurations): 4\n",
      "pqif values per simulation: 5\n",
      "Seeds per pqif: 50\n",
      "Parallelization strategy: 5 pqif values × 50 seeds = 250 parallel processes per simulation\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 1: vt=0, vrest=-8.5\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 125.5min remaining: 188.2min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 125.5min remaining: 83.6min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 125.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 1 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 2: vt=0, vrest=-17\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 138.6min remaining: 207.9min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 138.8min remaining: 92.5min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 138.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 2 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 3: vt=0, vrest=-12.3\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 152.1min remaining: 228.2min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 152.2min remaining: 101.5min\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 152.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 3 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 4: vt=0, vrest=-22\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed: 163.9min remaining: 245.9min\n",
      "[Parallel(n_jobs=5)]: Done   3 out of   5 | elapsed: 164.3min remaining: 109.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# ✓ SIMULATION 4 COMPLETED\n",
      "######################################################################\n",
      "\n",
      "\n",
      "======================================================================\n",
      "¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed: 164.3min finished\n"
     ]
    }
   ],
   "source": [
    "# ========== Oscillations (parallelized) ==========\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "####### Global parameters #######\n",
    "N = 200\n",
    "N2 = int(N/2)\n",
    "p = 0.3\n",
    "gsyn = 0.5\n",
    "alpha = 0.25\n",
    "dt = 0.1\n",
    "itmax = 1000\n",
    "sigman = 1\n",
    "itstim = 200\n",
    "amp_corriente = 20\n",
    "amp0 = 4 # changed from 8 to 4\n",
    "nloop = 16\n",
    "nloop_train = 10\n",
    "cant_seed = 50\n",
    "ts = 5\n",
    "b = 1 / ts\n",
    "ftrain = 1\n",
    "\n",
    "####### File organization functions #######\n",
    "def crear_subcarpeta(nombre_carpeta, nombre_subcarpeta):\n",
    "    subcarpeta_path_total = (os.path.join(nombre_carpeta, nombre_subcarpeta))\n",
    "    if not os.path.exists(subcarpeta_path_total):\n",
    "        os.makedirs(subcarpeta_path_total)\n",
    "    return subcarpeta_path_total\n",
    "\n",
    "\n",
    "def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "    ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "    if not os.path.exists(ruta):\n",
    "        os.makedirs(ruta)\n",
    "    return ruta\n",
    "\n",
    "def crear_carpetas(num_simulacion): \n",
    "    # Main simulation folder\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "\n",
    "    # Subfolders within the simulation\n",
    "    sub_act = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "\n",
    "    return nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes\n",
    "\n",
    "\n",
    "def crear_archivo_parametros(filename_resultados, num_simulacion, nombre_carpeta, b, vt, vrest):\n",
    "    # Save simulation parameters to file\n",
    "    data_parametros = {\n",
    "        'N': [N],\n",
    "        'p': [p],\n",
    "        'gsyn': [gsyn],\n",
    "        'nloop': [nloop],\n",
    "        'nloop_train':[nloop_train],\n",
    "        'cant_seed': [cant_seed],\n",
    "        'dt': [dt],\n",
    "        'itmax': [itmax],\n",
    "        'itstim': [itstim],\n",
    "        'amp_corriente': [amp_corriente],\n",
    "        'amp0': [amp0],\n",
    "        'ftrain': [ftrain],\n",
    "        'alpha': [alpha],\n",
    "        'sigman': [sigman],\n",
    "        'vt': [vt],\n",
    "        'b': [b],\n",
    "        'vrest': [vrest],\n",
    "        'results_file': [filename_resultados],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_parametros)\n",
    "    filename_parametros = f'simulation_{num_simulacion}_parameters.csv'\n",
    "    csv_parametros_path = os.path.join(nombre_carpeta, filename_parametros)\n",
    "    df.to_csv(csv_parametros_path, index=False)\n",
    "\n",
    "####### Function to generate target patterns #######\n",
    "\n",
    "def generate_target(romega1, romega2, amp0):\n",
    "    target=np.zeros((N,itmax))\n",
    "    amp=np.random.uniform(size=N)*amp0\n",
    "    phase=np.random.uniform(0,2*np.pi,size=N)\n",
    "    indices = [i for i in range(N)]\n",
    "    indices = np.random.permutation(indices) # Indices to identify which neuron is assigned each frequency\n",
    "    \n",
    "    romega_vec = np.zeros(N)\n",
    "    \n",
    "    for i in range(N2):\n",
    "        romega_vec[indices[i]]= romega1\n",
    "        romega_vec[indices[i+N2]]=romega2\n",
    "    \n",
    "    omega=romega_vec*2*np.pi/itmax\n",
    "\n",
    "    for it in range(itmax):\n",
    "        target[:,it]=amp*np.cos(it*omega+phase) \n",
    "            \n",
    "    return target, amp, phase, omega, romega_vec, amp0\n",
    "\n",
    "\n",
    "def save_target(target, phase, omega, romega_vec, amp, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "    # Save target parameters and values to CSV\n",
    "    data = {'Neurona': range(N), 'Fase': phase, 'Frecuencia': omega, 'romega': romega_vec, 'Amplitud': amp, 'amp0': amp0}\n",
    "    df = pd.DataFrame(data)\n",
    "    nombre_archivo = f'simulation_{num_simulacion}_targets_parameters.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "    df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "    target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "    nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "    target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "\n",
    "def guardar_matriz_csv(matriz, nombre_archivo):\n",
    "    with open(nombre_archivo, 'w', newline='') as archivo_csv:\n",
    "        escritor_csv = csv.writer(archivo_csv)\n",
    "        for fila in matriz:\n",
    "            fila_lista = [str(elemento) for elemento in fila.flat]\n",
    "            escritor_csv.writerow(fila_lista)\n",
    "\n",
    "\n",
    "####### Dynamics and learning functions #######\n",
    "\n",
    "def dynamics(x_var,r_var,I_var,nqif, b):\n",
    "    dx=np.zeros(N)\n",
    "    I_noise_lif = np.random.randn(N - nqif)*sigman \n",
    "    I_noise_qif = np.random.randn(nqif)*sigman\n",
    "    # LIF neurons\n",
    "    dx[nqif:] = -x_var[nqif:] + I_var[nqif:] + I_noise_lif\n",
    "    # QIF neurons\n",
    "    dx[:nqif] = 1 - np.cos(x_var[:nqif]) + I_var[:nqif]*(1 + np.cos(x_var[:nqif])) + I_noise_qif\n",
    "    dr = -b*r_var\n",
    "    return dx,dr\n",
    "\n",
    "\n",
    "def detect(x,xnew,rnew,nspike,nqif, b, vt, vrest):\n",
    "    # LIF spike detection\n",
    "    ispike_lif=np.where(x[nqif:]<vt) and np.where(xnew[nqif:]>vt)\n",
    "    ispike_lif=ispike_lif[0]+nqif\n",
    "    if(len(ispike_lif)>0):\n",
    "        rnew[ispike_lif[:]] = rnew[ispike_lif[:]] + b\n",
    "        xnew[ispike_lif[:]] = vrest\n",
    "        nspike[ispike_lif[:]] = nspike[ispike_lif[:]] + 1\n",
    "    # QIF spike detection\n",
    "    dpi=np.mod(np.pi - np.mod(x,2*np.pi),2*np.pi)  # distance to pi\n",
    "    ispike_qif=np.where((xnew[:nqif]-x[:nqif])>0) and np.where((xnew[:nqif]-x[:nqif]-dpi[:nqif])>0)\n",
    "    if(len(ispike_qif)>0):\n",
    "        rnew[ispike_qif[:]] = rnew[ispike_qif[:]] + b\n",
    "        nspike[ispike_qif[:]] = nspike[ispike_qif[:]] + 1\n",
    "    return xnew,rnew,nspike\n",
    "\n",
    "def evolution(x, r, Iext, w, nqif, it, dt, iout, nspike, b, vt, vrest):\n",
    "    II = np.squeeze(np.asarray(Iext[:, it]))\n",
    "    v = w.dot(r.T).A1\n",
    "    dx, dr = dynamics(x, r, II + v, nqif, b)\n",
    "    xnew = x + dt * dx / 2\n",
    "    rnew = r + dt * dr / 2\n",
    "    dx, dr = dynamics(xnew, rnew, II + v, nqif, b)\n",
    "    xnew = x + dt * dx\n",
    "    rnew = r + dt * dr\n",
    "    xnew, rnew, nspike = detect(x, xnew, rnew, nspike, nqif, b, vt, vrest)\n",
    "    x, r = np.copy(xnew), np.copy(rnew)\n",
    "\n",
    "    return x, r, nspike, r[iout], II, v\n",
    "\n",
    "\n",
    "def initialize_connectivity_matrix(N, p, gsyn):\n",
    "    w = sparse.random(N, N, p, data_rvs=np.random.randn).todense()\n",
    "    np.fill_diagonal(w, 0)  # No autapses\n",
    "    w *= gsyn / np.sqrt(p * N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        i0 = np.where(w[i, :])[1]\n",
    "        if len(i0) > 0:\n",
    "            av0 = np.sum(w[i, i0]) / len(i0)\n",
    "            w[i, i0] -= av0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def initialize_neurons(N):\n",
    "    x = np.random.uniform(size=N) * 2 * np.pi\n",
    "    r = np.zeros(N)\n",
    "    nspike = np.zeros(N)\n",
    "    return x, r, nspike\n",
    "\n",
    "def initialize_training(N, w):\n",
    "    # Initialize correlation matrices for RLS learning\n",
    "    nind=np.zeros(N).astype('int')\n",
    "    idx=[]\n",
    "    P=[]\n",
    "    for i in range(N):\n",
    "        ind=np.where(w[i,:])[1]\n",
    "        nind[i]=len(ind)\n",
    "        idx.append(ind)\n",
    "        P.append(np.identity(nind[i])/alpha)   \n",
    "    return P, idx\n",
    "\n",
    "def currents(N, itmax):\n",
    "    Iext=np.zeros((N,itmax))\n",
    "    Ibac=amp_corriente*(2*np.random.uniform(size=N)-1)\n",
    "    Iext[:, :itstim] = Ibac[:, None]  # Vectorized assignment\n",
    "    return Iext\n",
    "\n",
    "\n",
    "def learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer):\n",
    "    error = target[:, it:it + 1] - w @ r.reshape(N, 1)\n",
    "    for i in range(N):\n",
    "        ri = r[idx[i]].reshape(len(idx[i]), 1)\n",
    "        k1 = P[i] @ ri\n",
    "        k2 = ri.T @ P[i]\n",
    "        den = 1 + ri.T @ k1\n",
    "        P[i] -= (k1 @ k2) / den\n",
    "        dw = error[i, 0] * P[i] @ r[idx[i]]\n",
    "        w[i, idx[i]] += dw\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        modt_value = it + iloop * itmax\n",
    "        modw_value = np.log(np.linalg.norm(w) / norm_w0)\n",
    "        csv_writer.writerow([modt_value, modw_value])\n",
    "        \n",
    "    return w, P\n",
    "\n",
    "\n",
    "####### Motifs and dimensionality calculations #######\n",
    "            \n",
    "def motifs(w,gsyn,N):\n",
    "    w=w-np.mean(w)\n",
    "    \n",
    "    ww=np.matmul(w,w)\n",
    "    wtw=np.matmul(w.T,w)\n",
    "    wwt=np.matmul(w,w.T)\n",
    "    \n",
    "    sigma2=np.trace(wwt)/N\n",
    "    \n",
    "    tau_rec=np.trace(ww)\n",
    "    tau_rec/=sigma2*N\n",
    "    \n",
    "    tau_div=np.sum(wwt)-np.trace(wwt)\n",
    "    tau_div/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_con=np.sum(wtw)-np.trace(wtw)\n",
    "    tau_con/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_chn=2*(np.sum(ww)-np.trace(ww))\n",
    "    tau_chn/=sigma2*N*(N-1)\n",
    "    \n",
    "    return sigma2,tau_rec,tau_div,tau_con,tau_chn\n",
    "\n",
    "\n",
    "####### Parallelized simulation functions #######\n",
    "\n",
    "def run_single_seed(seed, pqif, num_simulacion, vt, vrest, target, \n",
    "                    N, N2, p, gsyn, nloop, nloop_train,\n",
    "                    dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                    b, iout, nombre_carpeta, sub_pesos, sub_corrientes, \n",
    "                    sub_inputs, sub_outputs, sub_nspikes):\n",
    "    \"\"\"\n",
    "    Run complete simulation for a single seed\n",
    "    This function is parallelized over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate nqif based on proportion of QIF neurons\n",
    "    nqif = int(N * pqif)\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    # Initialize network\n",
    "    x, r, nspike = initialize_neurons(N)\n",
    "    Iext = currents(N, itmax)\n",
    "    \n",
    "    # Save external current\n",
    "    path_Iext = os.path.join(nombre_carpeta, \n",
    "                            f'simulation_{num_simulacion}_Iext_pqif_{pqif}_seed_{seed}.csv')\n",
    "    guardar_matriz_csv(Iext, path_Iext)\n",
    "    \n",
    "    # Initialize connectivity\n",
    "    w = initialize_connectivity_matrix(N, p, gsyn)\n",
    "    norm_w0 = np.linalg.norm(w)\n",
    "    P, idx = initialize_training(N, w)\n",
    "    \n",
    "    # Prepare file for weight evolution tracking\n",
    "    filename_dw = os.path.join(nombre_carpeta, \n",
    "                              f'simulation_{num_simulacion}_dw_pqif_{pqif}_seed_{seed}.csv')\n",
    "    \n",
    "    # Storage for results across all loops\n",
    "    seed_results = []\n",
    "    corrientes_buffer = []\n",
    "    \n",
    "    with open(filename_dw, mode='w', newline='') as file_dw:\n",
    "        csv_writer_dw = csv.writer(file_dw)\n",
    "        csv_writer_dw.writerow(['modt', 'modw'])\n",
    "        \n",
    "        # Main training loop\n",
    "        for iloop in range(nloop):\n",
    "            \n",
    "            # Pre-allocate arrays for this loop\n",
    "            outputs_loop = []\n",
    "            inputs_loop = []\n",
    "            nspikes_loop = []\n",
    "            \n",
    "            # Define output paths\n",
    "            path_inputs = os.path.join(sub_inputs, \n",
    "                                      f'simulation_{num_simulacion}_inputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_nspikes = os.path.join(sub_nspikes, \n",
    "                                       f'simulation_{num_simulacion}_nspikes_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_outputs = os.path.join(sub_outputs, \n",
    "                                       f'simulation_{num_simulacion}_outputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            \n",
    "            # Time evolution for this loop\n",
    "            for it in range(itmax):\n",
    "                nspike = np.zeros(N)\n",
    "                \n",
    "                x, r, nspike, rout, II, v = evolution(x, r, Iext, w, nqif, it, dt, \n",
    "                                                     iout, nspike, b, vt=vt, vrest=vrest)\n",
    "                \n",
    "                entrada = II + v\n",
    "                \n",
    "                # Accumulate data in memory (more efficient than writing each iteration)\n",
    "                outputs_loop.append(rout)\n",
    "                inputs_loop.append(entrada)\n",
    "                nspikes_loop.append(nspike)\n",
    "                \n",
    "                # Record currents at specific time points in specific loops\n",
    "                if iloop in [nloop_train + 1, nloop - 1] and it % 20 == 0:\n",
    "                    corrientes_buffer.append([pqif, seed, iloop, it, \n",
    "                                            II[0], v[0], II[1], v[1], \n",
    "                                            II[N2+1], v[N2+1], II[N2+2], v[N2+2]])\n",
    "                \n",
    "                # Apply learning rule during training period\n",
    "                if iloop > 0 and iloop <= nloop_train and int(it > itstim):\n",
    "                    w, P = learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer_dw)\n",
    "            \n",
    "            # Save all data for this loop (single write per loop)\n",
    "            np.savetxt(path_inputs, np.array(inputs_loop), delimiter=',')\n",
    "            np.savetxt(path_nspikes, np.array(nspikes_loop), delimiter=',')\n",
    "            np.savetxt(path_outputs, np.array(outputs_loop), delimiter=',')\n",
    "            \n",
    "            # Calculate network motifs\n",
    "            sigma2, tau_rec, tau_div, tau_con, tau_chn = motifs(w, gsyn, N)\n",
    "            \n",
    "            # Save weight matrix at specific loops\n",
    "            if iloop == 0 or iloop == (nloop_train + 1):\n",
    "                path_w_seed = os.path.join(sub_pesos, \n",
    "                                          f'simulation_{num_simulacion}_connectivity_pqif_{pqif}_iloop_{iloop}_seed_{seed}')\n",
    "                guardar_matriz_csv(w, path_w_seed)\n",
    "            \n",
    "            # Store results for this loop\n",
    "            seed_results.append([pqif, seed, iloop, sigma2, tau_rec, \n",
    "                               tau_div, tau_con, tau_chn])\n",
    "    \n",
    "    return seed_results, corrientes_buffer\n",
    "\n",
    "\n",
    "def run_pqif_simulation(pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0, \n",
    "                        N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                        dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                        b, iout):\n",
    "    \"\"\"\n",
    "    Run simulation for specific pqif value, parallelizing over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simulation {num_simulacion} - Processing pqif = {pqif}\")\n",
    "    print(f\"vt={vt}, vrest={vrest}\")\n",
    "    print(f\"Parallelizing over {cant_seed} seeds\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get folder paths (already created)\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    sub_act = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "    \n",
    "    # Results file path\n",
    "    filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "    csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "    \n",
    "    # Prepare currents file header\n",
    "    path_currents_seed = os.path.join(sub_corrientes, \n",
    "                                       f'simulation_{num_simulacion}_currents_pqif_{pqif}.csv')\n",
    "    with open(path_currents_seed, mode='w', newline='') as file_:\n",
    "        writer_ = csv.writer(file_)\n",
    "        writer_.writerow(['pqif', 'seed', 'iloop', 'it', 'II_0', 'v_0', \n",
    "                        'II_1', 'v_1', 'II_N2+1', 'v_N2+1', 'II_N2+2', 'v_N2+2'])\n",
    "    \n",
    "    # Parallelize over seeds\n",
    "    results = Parallel(n_jobs=cant_seed, verbose=5)(\n",
    "        delayed(run_single_seed)(\n",
    "            seed, pqif, num_simulacion, vt, vrest, target,\n",
    "            N, N2, p, gsyn, nloop, nloop_train,\n",
    "            dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "            b, iout, nombre_carpeta, sub_pesos, sub_corrientes,\n",
    "            sub_inputs, sub_outputs, sub_nspikes\n",
    "        )\n",
    "        for seed in range(cant_seed)\n",
    "    )\n",
    "    \n",
    "    # Consolidate results from all seeds\n",
    "    # results is a list of (seed_results, corrientes_buffer) tuples\n",
    "    all_seed_results = []\n",
    "    all_corrientes = []\n",
    "    for seed_results, corrientes_buffer in results:\n",
    "        all_seed_results.extend(seed_results)\n",
    "        all_corrientes.extend(corrientes_buffer)\n",
    "    \n",
    "    # Write all results to file (append mode for this pqif)\n",
    "    with open(csv_file_path, 'a', newline='') as file_res:\n",
    "        writer_res = csv.writer(file_res)\n",
    "        writer_res.writerows(all_seed_results)\n",
    "    \n",
    "    # Write all currents to file\n",
    "    if all_corrientes:\n",
    "        with open(path_currents_seed, 'a', newline='') as f_corr:\n",
    "            writer_corr = csv.writer(f_corr)\n",
    "            writer_corr.writerows(all_corrientes)\n",
    "    \n",
    "    print(f\"✓ pqif={pqif} completed for simulation {num_simulacion}\\n\")\n",
    "    \n",
    "    return num_simulacion\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    iout = np.linspace(0, N, num=N, endpoint=False).astype('int')\n",
    "    \n",
    "    # Generate target pattern once (shared across all simulations)\n",
    "    print(\"Generating targets...\")\n",
    "    target, phase, amp, omega, romega_vec, amp0 = generate_target(romega1=1, romega2=5, amp0=amp0)\n",
    "    print(\"✓ Targets generated\\n\")\n",
    "    \n",
    "    # Define pqif values to simulate\n",
    "    pqif_values = [0, 0.25, 0.5, 0.75, 1]  # pqif values are added here\n",
    "    \n",
    "    # Define vt/vrest configurations (defines simulation number)\n",
    "    # Each simulation uses different reset parameters for LIF neurons\n",
    "    # QIF neurons always use vt=None, vrest=None (handled in dynamics)\n",
    "    configs = [\n",
    "        {'vt': 0, 'vrest': -8.5},  # Simulation 1\n",
    "        {'vt': 0, 'vrest': -17},    # Simulation 2\n",
    "        {'vt': 0, 'vrest': -12.3},  # Simulation 3\n",
    "        {'vt': 0, 'vrest': -22},  # Simulation 4\n",
    "\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total simulations (vt/vrest configurations): {len(configs)}\")\n",
    "    print(f\"pqif values per simulation: {len(pqif_values)}\")\n",
    "    print(f\"Seeds per pqif: {cant_seed}\")\n",
    "    print(f\"Parallelization strategy: {len(pqif_values)} pqif values × {cant_seed} seeds = {len(pqif_values)*cant_seed} parallel processes per simulation\\n\")\n",
    "    \n",
    "    # Iterate over each vt/vrest configuration\n",
    "    for num_simulacion, config in enumerate(configs, start=1):\n",
    "        vt = config['vt']\n",
    "        vrest = config['vrest']\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# STARTING SIMULATION {num_simulacion}: vt={vt}, vrest={vrest}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        # Create folders and files for this simulation\n",
    "        nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes = crear_carpetas(num_simulacion)\n",
    "        \n",
    "        # Save targets for each pqif (all share the same target)\n",
    "        for pqif in pqif_values:\n",
    "            save_target(target, phase=phase, omega=omega, romega_vec=romega_vec, \n",
    "                       amp=amp, amp0=amp0, num_simulacion=num_simulacion, \n",
    "                       nombre_carpeta=nombre_carpeta, pqif=pqif)\n",
    "        \n",
    "        # Create parameters file\n",
    "        filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "        crear_archivo_parametros(filename_resultados, num_simulacion, \n",
    "                                nombre_carpeta, b, vt, vrest=vrest)\n",
    "        \n",
    "        # Create results file with header\n",
    "        csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['pqif', 'seed', 'nloop', 'sigma2', 'tau_rec',\n",
    "                           'tau_div', 'tau_con', 'tau_chn'])\n",
    "        \n",
    "        # Parallelize over pqif for this simulation\n",
    "        # Each pqif will internally parallelize over seeds\n",
    "        results = Parallel(n_jobs=len(pqif_values), verbose=10)(\n",
    "            delayed(run_pqif_simulation)(\n",
    "                pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0,\n",
    "                N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                b, iout\n",
    "            )\n",
    "            for pqif in pqif_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ✓ SIMULATION {num_simulacion} COMPLETED\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ecd825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating targets...\n",
      "✓ Targets generated\n",
      "\n",
      "Total simulations (vt/vrest configurations): 4\n",
      "pqif values per simulation: 5\n",
      "Seeds per pqif: 50\n",
      "Parallelization strategy: 5 pqif values × 50 seeds = 250 parallel processes per simulation\n",
      "\n",
      "\n",
      "######################################################################\n",
      "# STARTING SIMULATION 9: vt=0, vrest=-8.5\n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 544\u001b[39m\n\u001b[32m    538\u001b[39m     writer.writerow([\u001b[33m'\u001b[39m\u001b[33mpqif\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnloop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msigma2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtau_rec\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    539\u001b[39m                    \u001b[33m'\u001b[39m\u001b[33mtau_div\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtau_con\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtau_chn\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m# Parallelize over pqif for this simulation\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[38;5;66;03m# Each pqif will internally parallelize over seeds\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpqif_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_pqif_simulation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpqif\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulacion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvrest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mromega_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgsyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnloop_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcant_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitstim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp_corriente\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigman\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miout\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpqif\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpqif_values\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m#\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    555\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# ✓ SIMULATION \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_simulacion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m COMPLETED\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ========== Sequences (parallelized) ==========\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "####### Global parameters #######\n",
    "N = 200\n",
    "N2 = int(N/2)\n",
    "p = 0.3\n",
    "gsyn = 0.5\n",
    "alpha = 0.25\n",
    "dt = 0.1\n",
    "itmax = 1000\n",
    "sigman = 1\n",
    "itstim = 200\n",
    "amp_corriente = 20\n",
    "amp0 = 4 # changed from 8 to 4\n",
    "sg_index = 0.15 # TODO Not sure what this is, but is a parameter in sequences \n",
    "nloop = 16\n",
    "nloop_train = 10\n",
    "cant_seed = 50\n",
    "ts = 5\n",
    "b = 1 / ts\n",
    "ftrain = 1\n",
    "\n",
    "####### File organization functions #######\n",
    "def crear_subcarpeta(nombre_carpeta, nombre_subcarpeta):\n",
    "    subcarpeta_path_total = (os.path.join(nombre_carpeta, nombre_subcarpeta))\n",
    "    if not os.path.exists(subcarpeta_path_total):\n",
    "        os.makedirs(subcarpeta_path_total)\n",
    "    return subcarpeta_path_total\n",
    "\n",
    "\n",
    "def crear_subcarpeta(carpeta_padre, nombre_subcarpeta):\n",
    "    ruta = os.path.join(carpeta_padre, nombre_subcarpeta)\n",
    "    if not os.path.exists(ruta):\n",
    "        os.makedirs(ruta)\n",
    "    return ruta\n",
    "\n",
    "def crear_carpetas(num_simulacion): \n",
    "    # Main simulation folder\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    if not os.path.exists(nombre_carpeta):\n",
    "        os.makedirs(nombre_carpeta)\n",
    "\n",
    "    # Subfolders within the simulation\n",
    "    sub_act = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = crear_subcarpeta(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "\n",
    "    return nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes\n",
    "\n",
    "\n",
    "def crear_archivo_parametros(filename_resultados, num_simulacion, nombre_carpeta, b, vt, vrest):\n",
    "    # Save simulation parameters to file\n",
    "    data_parametros = {\n",
    "        'N': [N],\n",
    "        'p': [p],\n",
    "        'gsyn': [gsyn],\n",
    "        'nloop': [nloop],\n",
    "        'nloop_train':[nloop_train],\n",
    "        'cant_seed': [cant_seed],\n",
    "        'dt': [dt],\n",
    "        'itmax': [itmax],\n",
    "        'itstim': [itstim],\n",
    "        'amp_corriente': [amp_corriente],\n",
    "        'amp0': [amp0],\n",
    "        'ftrain': [ftrain],\n",
    "        'alpha': [alpha],\n",
    "        'sigman': [sigman],\n",
    "        'vt': [vt],\n",
    "        'b': [b],\n",
    "        'vrest': [vrest],\n",
    "        'results_file': [filename_resultados],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_parametros)\n",
    "    filename_parametros = f'simulation_{num_simulacion}_parameters.csv'\n",
    "    csv_parametros_path = os.path.join(nombre_carpeta, filename_parametros)\n",
    "    df.to_csv(csv_parametros_path, index=False)\n",
    "\n",
    "####### Function to generate target patterns #######\n",
    "\n",
    "### Sequences version (I believe this also saves target)\n",
    "# Needs to change to english\n",
    "\n",
    "# def generate_target(num_simulacion, nombre_carpeta,sg_index, amp0, pqif):\n",
    "def generate_target(sg_index, amp0):\n",
    "\n",
    "    target = np.zeros(shape=(N,itmax))\n",
    "    #para secuencias periodicas\n",
    "\n",
    "    gg=np.zeros(N)\n",
    "    sg=sg_index*N            # ancho de la gaussiana. trelativo al tamanio del sistema\n",
    "    omegagauss=0.1       # velocidad de desplazamiento\n",
    "    for i in range(N):\n",
    "        gg[i]=amp0*np.exp(-(i-N/2)**2/(2*sg**2))\n",
    "    for it in range(itmax):\n",
    "        target[:,it]=np.roll(gg,int(omegagauss*it))\n",
    "\n",
    "    return target, omegagauss\n",
    "\n",
    "# def save_target(target, omegagauss, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "def save_target(target, phase, omega, romega_vec, amp, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "    target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "    nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "    target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "    data = {'sg_index': sg_index, 'omegagauss': omegagauss, 'amp0': amp0}\n",
    "    df = pd.DataFrame(data, index=[0])\n",
    "    nombre_archivo = f'simulation_{num_simulacion}_targets_parametros.csv'\n",
    "    csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "    df.to_csv(csv_target_path, index=False)\n",
    "            \n",
    "    return target \n",
    "\n",
    "#### Oscillations (TODO REMOVE THIS)\n",
    "# def generate_target(romega1, romega2, amp0):\n",
    "#     target=np.zeros((N,itmax))\n",
    "#     amp=np.random.uniform(size=N)*amp0\n",
    "#     phase=np.random.uniform(0,2*np.pi,size=N)\n",
    "#     indices = [i for i in range(N)]\n",
    "#     indices = np.random.permutation(indices) # Indices to identify which neuron is assigned each frequency\n",
    "    \n",
    "#     romega_vec = np.zeros(N)\n",
    "    \n",
    "#     for i in range(N2):\n",
    "#         romega_vec[indices[i]]= romega1\n",
    "#         romega_vec[indices[i+N2]]=romega2\n",
    "    \n",
    "#     omega=romega_vec*2*np.pi/itmax\n",
    "\n",
    "#     for it in range(itmax):\n",
    "#         target[:,it]=amp*np.cos(it*omega+phase) \n",
    "            \n",
    "#     return target, amp, phase, omega, romega_vec, amp0\n",
    "\n",
    "\n",
    "# def save_target(target, phase, omega, romega_vec, amp, amp0, num_simulacion, nombre_carpeta, pqif):\n",
    "#     # Save target parameters and values to CSV\n",
    "#     data = {'Neurona': range(N), 'Fase': phase, 'Frecuencia': omega, 'romega': romega_vec, 'Amplitud': amp, 'amp0': amp0}\n",
    "#     df = pd.DataFrame(data)\n",
    "#     nombre_archivo = f'simulation_{num_simulacion}_targets_parameters.csv'\n",
    "#     csv_target_path = os.path.join(nombre_carpeta, nombre_archivo)\n",
    "#     df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "#     target_df = pd.DataFrame(target.T, columns=[f'Neurona_{i}' for i in range(N)])\n",
    "#     nombre_archivo_target = f'simulation_{num_simulacion}_targets_{pqif}.csv'\n",
    "#     csv_target_path = os.path.join(nombre_carpeta, nombre_archivo_target)\n",
    "#     target_df.to_csv(csv_target_path, index=False)\n",
    "\n",
    "\n",
    "def guardar_matriz_csv(matriz, nombre_archivo):\n",
    "    with open(nombre_archivo, 'w', newline='') as archivo_csv:\n",
    "        escritor_csv = csv.writer(archivo_csv)\n",
    "        for fila in matriz:\n",
    "            fila_lista = [str(elemento) for elemento in fila.flat]\n",
    "            escritor_csv.writerow(fila_lista)\n",
    "\n",
    "\n",
    "####### Dynamics and learning functions #######\n",
    "\n",
    "def dynamics(x_var,r_var,I_var,nqif, b):\n",
    "    dx=np.zeros(N)\n",
    "    I_noise_lif = np.random.randn(N - nqif)*sigman \n",
    "    I_noise_qif = np.random.randn(nqif)*sigman\n",
    "    # LIF neurons\n",
    "    dx[nqif:] = -x_var[nqif:] + I_var[nqif:] + I_noise_lif\n",
    "    # QIF neurons\n",
    "    dx[:nqif] = 1 - np.cos(x_var[:nqif]) + I_var[:nqif]*(1 + np.cos(x_var[:nqif])) + I_noise_qif\n",
    "    dr = -b*r_var\n",
    "    return dx,dr\n",
    "\n",
    "\n",
    "def detect(x,xnew,rnew,nspike,nqif, b, vt, vrest):\n",
    "    # LIF spike detection\n",
    "    ispike_lif=np.where(x[nqif:]<vt) and np.where(xnew[nqif:]>vt)\n",
    "    ispike_lif=ispike_lif[0]+nqif\n",
    "    if(len(ispike_lif)>0):\n",
    "        rnew[ispike_lif[:]] = rnew[ispike_lif[:]] + b\n",
    "        xnew[ispike_lif[:]] = vrest\n",
    "        nspike[ispike_lif[:]] = nspike[ispike_lif[:]] + 1\n",
    "    # QIF spike detection\n",
    "    dpi=np.mod(np.pi - np.mod(x,2*np.pi),2*np.pi)  # distance to pi\n",
    "    ispike_qif=np.where((xnew[:nqif]-x[:nqif])>0) and np.where((xnew[:nqif]-x[:nqif]-dpi[:nqif])>0)\n",
    "    if(len(ispike_qif)>0):\n",
    "        rnew[ispike_qif[:]] = rnew[ispike_qif[:]] + b\n",
    "        nspike[ispike_qif[:]] = nspike[ispike_qif[:]] + 1\n",
    "    return xnew,rnew,nspike\n",
    "\n",
    "def evolution(x, r, Iext, w, nqif, it, dt, iout, nspike, b, vt, vrest):\n",
    "    II = np.squeeze(np.asarray(Iext[:, it]))\n",
    "    v = w.dot(r.T).A1\n",
    "    dx, dr = dynamics(x, r, II + v, nqif, b)\n",
    "    xnew = x + dt * dx / 2\n",
    "    rnew = r + dt * dr / 2\n",
    "    dx, dr = dynamics(xnew, rnew, II + v, nqif, b)\n",
    "    xnew = x + dt * dx\n",
    "    rnew = r + dt * dr\n",
    "    xnew, rnew, nspike = detect(x, xnew, rnew, nspike, nqif, b, vt, vrest)\n",
    "    x, r = np.copy(xnew), np.copy(rnew)\n",
    "\n",
    "    return x, r, nspike, r[iout], II, v\n",
    "\n",
    "\n",
    "def initialize_connectivity_matrix(N, p, gsyn):\n",
    "    w = sparse.random(N, N, p, data_rvs=np.random.randn).todense()\n",
    "    np.fill_diagonal(w, 0)  # No autapses\n",
    "    w *= gsyn / np.sqrt(p * N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        i0 = np.where(w[i, :])[1]\n",
    "        if len(i0) > 0:\n",
    "            av0 = np.sum(w[i, i0]) / len(i0)\n",
    "            w[i, i0] -= av0\n",
    "    \n",
    "    return w\n",
    "\n",
    "def initialize_neurons(N):\n",
    "    x = np.random.uniform(size=N) * 2 * np.pi\n",
    "    r = np.zeros(N)\n",
    "    nspike = np.zeros(N)\n",
    "    return x, r, nspike\n",
    "\n",
    "def initialize_training(N, w):\n",
    "    # Initialize correlation matrices for RLS learning\n",
    "    nind=np.zeros(N).astype('int')\n",
    "    idx=[]\n",
    "    P=[]\n",
    "    for i in range(N):\n",
    "        ind=np.where(w[i,:])[1]\n",
    "        nind[i]=len(ind)\n",
    "        idx.append(ind)\n",
    "        P.append(np.identity(nind[i])/alpha)   \n",
    "    return P, idx\n",
    "\n",
    "def currents(N, itmax):\n",
    "    Iext=np.zeros((N,itmax))\n",
    "    Ibac=amp_corriente*(2*np.random.uniform(size=N)-1)\n",
    "    Iext[:, :itstim] = Ibac[:, None]  # Vectorized assignment\n",
    "    return Iext\n",
    "\n",
    "\n",
    "def learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer):\n",
    "    error = target[:, it:it + 1] - w @ r.reshape(N, 1)\n",
    "    for i in range(N):\n",
    "        ri = r[idx[i]].reshape(len(idx[i]), 1)\n",
    "        k1 = P[i] @ ri\n",
    "        k2 = ri.T @ P[i]\n",
    "        den = 1 + ri.T @ k1\n",
    "        P[i] -= (k1 @ k2) / den\n",
    "        dw = error[i, 0] * P[i] @ r[idx[i]]\n",
    "        w[i, idx[i]] += dw\n",
    "\n",
    "    if it % 10 == 0:\n",
    "        modt_value = it + iloop * itmax\n",
    "        modw_value = np.log(np.linalg.norm(w) / norm_w0)\n",
    "        csv_writer.writerow([modt_value, modw_value])\n",
    "        \n",
    "    return w, P\n",
    "\n",
    "\n",
    "####### Motifs and dimensionality calculations #######\n",
    "            \n",
    "def motifs(w,gsyn,N):\n",
    "    w=w-np.mean(w)\n",
    "    \n",
    "    ww=np.matmul(w,w)\n",
    "    wtw=np.matmul(w.T,w)\n",
    "    wwt=np.matmul(w,w.T)\n",
    "    \n",
    "    sigma2=np.trace(wwt)/N\n",
    "    \n",
    "    tau_rec=np.trace(ww)\n",
    "    tau_rec/=sigma2*N\n",
    "    \n",
    "    tau_div=np.sum(wwt)-np.trace(wwt)\n",
    "    tau_div/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_con=np.sum(wtw)-np.trace(wtw)\n",
    "    tau_con/=sigma2*N*(N-1)\n",
    "    \n",
    "    tau_chn=2*(np.sum(ww)-np.trace(ww))\n",
    "    tau_chn/=sigma2*N*(N-1)\n",
    "    \n",
    "    return sigma2,tau_rec,tau_div,tau_con,tau_chn\n",
    "\n",
    "\n",
    "####### Parallelized simulation functions #######\n",
    "\n",
    "def run_single_seed(seed, pqif, num_simulacion, vt, vrest, target, \n",
    "                    N, N2, p, gsyn, nloop, nloop_train,\n",
    "                    dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                    b, iout, nombre_carpeta, sub_pesos, sub_corrientes, \n",
    "                    sub_inputs, sub_outputs, sub_nspikes):\n",
    "    \"\"\"\n",
    "    Run complete simulation for a single seed\n",
    "    This function is parallelized over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate nqif based on proportion of QIF neurons\n",
    "    nqif = int(N * pqif)\n",
    "    \n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    # Initialize network\n",
    "    x, r, nspike = initialize_neurons(N)\n",
    "    Iext = currents(N, itmax)\n",
    "    \n",
    "    # Save external current\n",
    "    path_Iext = os.path.join(nombre_carpeta, \n",
    "                            f'simulation_{num_simulacion}_Iext_pqif_{pqif}_seed_{seed}.csv')\n",
    "    guardar_matriz_csv(Iext, path_Iext)\n",
    "    \n",
    "    # Initialize connectivity\n",
    "    w = initialize_connectivity_matrix(N, p, gsyn)\n",
    "    norm_w0 = np.linalg.norm(w)\n",
    "    P, idx = initialize_training(N, w)\n",
    "    \n",
    "    # Prepare file for weight evolution tracking\n",
    "    filename_dw = os.path.join(nombre_carpeta, \n",
    "                              f'simulation_{num_simulacion}_dw_pqif_{pqif}_seed_{seed}.csv')\n",
    "    \n",
    "    # Storage for results across all loops\n",
    "    seed_results = []\n",
    "    corrientes_buffer = []\n",
    "    \n",
    "    with open(filename_dw, mode='w', newline='') as file_dw:\n",
    "        csv_writer_dw = csv.writer(file_dw)\n",
    "        csv_writer_dw.writerow(['modt', 'modw'])\n",
    "        \n",
    "        # Main training loop\n",
    "        for iloop in range(nloop):\n",
    "            \n",
    "            # Pre-allocate arrays for this loop\n",
    "            outputs_loop = []\n",
    "            inputs_loop = []\n",
    "            nspikes_loop = []\n",
    "            \n",
    "            # Define output paths\n",
    "            path_inputs = os.path.join(sub_inputs, \n",
    "                                      f'simulation_{num_simulacion}_inputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_nspikes = os.path.join(sub_nspikes, \n",
    "                                       f'simulation_{num_simulacion}_nspikes_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            path_outputs = os.path.join(sub_outputs, \n",
    "                                       f'simulation_{num_simulacion}_outputs_pqif_{pqif}_iloop_{iloop}_seed_{seed}.csv')\n",
    "            \n",
    "            # Time evolution for this loop\n",
    "            for it in range(itmax):\n",
    "                nspike = np.zeros(N)\n",
    "                \n",
    "                x, r, nspike, rout, II, v = evolution(x, r, Iext, w, nqif, it, dt, \n",
    "                                                     iout, nspike, b, vt=vt, vrest=vrest)\n",
    "                \n",
    "                entrada = II + v\n",
    "                \n",
    "                # Accumulate data in memory (more efficient than writing each iteration)\n",
    "                outputs_loop.append(rout)\n",
    "                inputs_loop.append(entrada)\n",
    "                nspikes_loop.append(nspike)\n",
    "                \n",
    "                # Record currents at specific time points in specific loops\n",
    "                if iloop in [nloop_train + 1, nloop - 1] and it % 20 == 0:\n",
    "                    corrientes_buffer.append([pqif, seed, iloop, it, \n",
    "                                            II[0], v[0], II[1], v[1], \n",
    "                                            II[N2+1], v[N2+1], II[N2+2], v[N2+2]])\n",
    "                \n",
    "                # Apply learning rule during training period\n",
    "                if iloop > 0 and iloop <= nloop_train and int(it > itstim):\n",
    "                    w, P = learning(it, iloop, w, r, P, idx, target, norm_w0, csv_writer_dw)\n",
    "            \n",
    "            # Save all data for this loop (single write per loop)\n",
    "            np.savetxt(path_inputs, np.array(inputs_loop), delimiter=',')\n",
    "            np.savetxt(path_nspikes, np.array(nspikes_loop), delimiter=',')\n",
    "            np.savetxt(path_outputs, np.array(outputs_loop), delimiter=',')\n",
    "            \n",
    "            # Calculate network motifs\n",
    "            sigma2, tau_rec, tau_div, tau_con, tau_chn = motifs(w, gsyn, N)\n",
    "            \n",
    "            # Save weight matrix at specific loops\n",
    "            if iloop == 0 or iloop == (nloop_train + 1):\n",
    "                path_w_seed = os.path.join(sub_pesos, \n",
    "                                          f'simulation_{num_simulacion}_connectivity_pqif_{pqif}_iloop_{iloop}_seed_{seed}')\n",
    "                guardar_matriz_csv(w, path_w_seed)\n",
    "            \n",
    "            # Store results for this loop\n",
    "            seed_results.append([pqif, seed, iloop, sigma2, tau_rec, \n",
    "                               tau_div, tau_con, tau_chn])\n",
    "    \n",
    "    return seed_results, corrientes_buffer\n",
    "\n",
    "\n",
    "def run_pqif_simulation(pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0, \n",
    "                        N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                        dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                        b, iout):\n",
    "    \"\"\"\n",
    "    Run simulation for specific pqif value, parallelizing over seeds\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simulation {num_simulacion} - Processing pqif = {pqif}\")\n",
    "    print(f\"vt={vt}, vrest={vrest}\")\n",
    "    print(f\"Parallelizing over {cant_seed} seeds\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Get folder paths (already created)\n",
    "    nombre_carpeta = f\"simulation_{num_simulacion}\"\n",
    "    sub_act = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_activity_examples\")\n",
    "    sub_pesos = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_connectivity_matrix\")\n",
    "    sub_corrientes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_currents\")\n",
    "    sub_inputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_inputs\")\n",
    "    sub_outputs = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_outputs\")\n",
    "    sub_nspikes = os.path.join(nombre_carpeta, f\"simulation_{num_simulacion}_nspikes\")\n",
    "    \n",
    "    # Results file path\n",
    "    filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "    csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "    \n",
    "    # Prepare currents file header\n",
    "    path_currents_seed = os.path.join(sub_corrientes, \n",
    "                                       f'simulation_{num_simulacion}_currents_pqif_{pqif}.csv')\n",
    "    with open(path_currents_seed, mode='w', newline='') as file_:\n",
    "        writer_ = csv.writer(file_)\n",
    "        writer_.writerow(['pqif', 'seed', 'iloop', 'it', 'II_0', 'v_0', \n",
    "                        'II_1', 'v_1', 'II_N2+1', 'v_N2+1', 'II_N2+2', 'v_N2+2'])\n",
    "    \n",
    "    # Parallelize over seeds\n",
    "    results = Parallel(n_jobs=cant_seed, verbose=5)(\n",
    "        delayed(run_single_seed)(\n",
    "            seed, pqif, num_simulacion, vt, vrest, target,\n",
    "            N, N2, p, gsyn, nloop, nloop_train,\n",
    "            dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "            b, iout, nombre_carpeta, sub_pesos, sub_corrientes,\n",
    "            sub_inputs, sub_outputs, sub_nspikes\n",
    "        )\n",
    "        for seed in range(cant_seed)\n",
    "    )\n",
    "    \n",
    "    # Consolidate results from all seeds\n",
    "    # results is a list of (seed_results, corrientes_buffer) tuples\n",
    "    all_seed_results = []\n",
    "    all_corrientes = []\n",
    "    for seed_results, corrientes_buffer in results:\n",
    "        all_seed_results.extend(seed_results)\n",
    "        all_corrientes.extend(corrientes_buffer)\n",
    "    \n",
    "    # Write all results to file (append mode for this pqif)\n",
    "    with open(csv_file_path, 'a', newline='') as file_res:\n",
    "        writer_res = csv.writer(file_res)\n",
    "        writer_res.writerows(all_seed_results)\n",
    "    \n",
    "    # Write all currents to file\n",
    "    if all_corrientes:\n",
    "        with open(path_currents_seed, 'a', newline='') as f_corr:\n",
    "            writer_corr = csv.writer(f_corr)\n",
    "            writer_corr.writerows(all_corrientes)\n",
    "    \n",
    "    print(f\"✓ pqif={pqif} completed for simulation {num_simulacion}\\n\")\n",
    "    \n",
    "    return num_simulacion\n",
    "\n",
    "\n",
    "# ===== MAIN EXECUTION =====\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    iout = np.linspace(0, N, num=N, endpoint=False).astype('int')\n",
    "    \n",
    "    # Generate target pattern once (shared across all simulations)\n",
    "    print(\"Generating targets...\")\n",
    "    target, omegagauss = generate_target(sg_index, amp0=amp0)\n",
    "    # target = generate_target(num_simulacion, nombre_carpeta,sg_index, amp0, pqif=pqif)\n",
    "    print(\"✓ Targets generated\\n\")\n",
    "    \n",
    "    # Define pqif values to simulate\n",
    "    pqif_values = [0, 0.25, 0.5, 0.75, 1]  # pqif values are added here\n",
    "    \n",
    "    # Define vt/vrest configurations (defines simulation number)\n",
    "    # Each simulation uses different reset parameters for LIF neurons\n",
    "    # QIF neurons always use vt=None, vrest=None (handled in dynamics)\n",
    "    configs = [\n",
    "        {'vt': 0, 'vrest': -8.5},  # Simulation 1\n",
    "        {'vt': 0, 'vrest': -17},    # Simulation 2\n",
    "        {'vt': 0, 'vrest': -12.3},  # Simulation 3\n",
    "        {'vt': 0, 'vrest': -22},  # Simulation 4\n",
    "\n",
    "    ]\n",
    "    \n",
    "    print(f\"Total simulations (vt/vrest configurations): {len(configs)}\")\n",
    "    print(f\"pqif values per simulation: {len(pqif_values)}\")\n",
    "    print(f\"Seeds per pqif: {cant_seed}\")\n",
    "    print(f\"Parallelization strategy: {len(pqif_values)} pqif values × {cant_seed} seeds = {len(pqif_values)*cant_seed} parallel processes per simulation\\n\")\n",
    "    \n",
    "    # Iterate over each vt/vrest configuration\n",
    "    for num_simulacion, config in enumerate(configs, start=9):  # I tried to make start = 9 \n",
    "        vt = config['vt']\n",
    "        vrest = config['vrest']\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# STARTING SIMULATION {num_simulacion}: vt={vt}, vrest={vrest}\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "        \n",
    "        # Create folders and files for this simulation\n",
    "        nombre_carpeta, sub_act, sub_pesos, sub_corrientes, sub_inputs, sub_outputs, sub_nspikes = crear_carpetas(num_simulacion)\n",
    "        \n",
    "        # Save targets for each pqif (all share the same target)\n",
    "        # I commented this out because in the sequernces code, it seems to save target as part of generate_target function\n",
    "\n",
    "        phase = None\n",
    "        amp = None\n",
    "        omega = None\n",
    "        romega_vec = None  # They are not used in sequences I believe, but I'm not sure where to change the code, so I try with setting them to None for now\n",
    "\n",
    "        for pqif in pqif_values:\n",
    "            save_target(target, phase=phase, omega=omega, romega_vec=romega_vec, \n",
    "                       amp=amp, amp0=amp0, num_simulacion=num_simulacion, \n",
    "                       nombre_carpeta=nombre_carpeta, pqif=pqif)\n",
    "        \n",
    "        # Create parameters file\n",
    "        filename_resultados = f'simulation_{num_simulacion}_results.csv'\n",
    "        crear_archivo_parametros(filename_resultados, num_simulacion, \n",
    "                                nombre_carpeta, b, vt, vrest=vrest)\n",
    "        \n",
    "        # Create results file with header\n",
    "        csv_file_path = os.path.join(nombre_carpeta, filename_resultados)\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['pqif', 'seed', 'nloop', 'sigma2', 'tau_rec',\n",
    "                           'tau_div', 'tau_con', 'tau_chn'])\n",
    "        \n",
    "\n",
    "        # Parallelize over pqif for this simulation\n",
    "        # Each pqif will internally parallelize over seeds\n",
    "        results = Parallel(n_jobs=len(pqif_values), verbose=10)(\n",
    "            delayed(run_pqif_simulation)(\n",
    "                pqif, num_simulacion, vt, vrest, target, phase, amp, omega, romega_vec, amp0,\n",
    "                N, N2, p, gsyn, nloop, nloop_train, cant_seed,\n",
    "                dt, itmax, itstim, amp_corriente, alpha, sigman,\n",
    "                b, iout\n",
    "            )\n",
    "            for pqif in pqif_values\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# ✓ SIMULATION {num_simulacion} COMPLETED\")\n",
    "        print(f\"{'#'*70}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"¡ALL SIMULATIONS COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
